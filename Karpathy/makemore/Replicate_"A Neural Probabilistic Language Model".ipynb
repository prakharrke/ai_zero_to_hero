{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae446d99-d11c-403c-b681-0e8db7756e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/prakhardixit/.pyenv/versions/3.12.4/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/prakhardixit/.pyenv/versions/3.12.4/lib/python3.12/site-packages (from nltk) (4.67.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.5/288.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95c6b5a-9822-4c4f-8cd4-2aee95ebe96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/prakhardixit/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9521078-0c10-41ed-8e27-35e8b1c5ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c0f69e-9fb1-46f4-b815-fe54ddf11e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a24c8a-d95d-431d-b493-fac4041915e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0412055-f875-4b8a-ac78-4fac30a572a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da121eab-b331-43ed-92f2-4a5b0cfaefd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each sentence by removing non-alphabetic tokens\n",
    "cleaned_sentences = [\n",
    "    [word.lower() for word in sentence if word.isalpha()]\n",
    "    for sentence in sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce57f438-1ef1-4173-8931-09de9fa3eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create stoi and itos\n",
    "words = list(sorted(set([word for sentence in cleaned_sentences for word in sentence])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b5c6d8d-bc12-434b-a74a-48e86c376aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words is our vocab\n",
    "stoi = {}\n",
    "itos = {} \n",
    "\n",
    "for index, word in enumerate(words):\n",
    "    stoi[word] = index + 1\n",
    "    itos[index + 1] = word\n",
    "\n",
    "stoi['.'] = 0 \n",
    "itos[0] = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39a244f-858b-4f86-bb78-6deba341d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "block_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0e09a0-e21a-4170-8859-c54e1ff32672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([814703, 5]) torch.Size([814703])\n",
      "torch.Size([102509, 5]) torch.Size([102509])\n",
      "torch.Size([99593, 5]) torch.Size([99593])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_dataset(sentences):  \n",
    "  X, Y = [], []\n",
    "  for sentence in sentences:\n",
    "    if len(sentence) < 5:\n",
    "        continue\n",
    "\n",
    "    context = [0] * block_size\n",
    "    for word in sentence + ['.']:\n",
    "      ix = stoi[word]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      # print(' '.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix]\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "    \n",
    "random.seed(42)\n",
    "random.shuffle(cleaned_sentences)\n",
    "n1 = int(0.8*len(cleaned_sentences))\n",
    "n2 = int(0.9*len(cleaned_sentences))\n",
    "\n",
    "Xtr, Ytr = build_dataset(cleaned_sentences[:n1])\n",
    "Xdev, Ydev = build_dataset(cleaned_sentences[n1:n2])\n",
    "Xte, Yte = build_dataset(cleaned_sentences[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278164a7-2887-44eb-a727-266d35ee3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483646)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e3f3cf5-46af-473c-811b-6f7ed9996dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the embeddings \n",
    "n_embed = 50\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32ec034b-d375-4ae4-b061-20963e0a8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40235, 50])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "425e6836-f7f6-4ef0-80d4-312691ad301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the shape of a single input. \n",
    "# Each input has 5 words, the embedding of each word is 10 dimensional. Hence each input will be 50 dimensional long. block_size * n_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fd19787-9c3a-4206-adac-860ffd40baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 50])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the embedding of the first input in Xtr\n",
    "C[Xtr[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98a8f39e-5410-4cfd-a1e5-f5d456bbb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network will have 2 hidden layers to begin with. Each hidden layer will have 100 neurons. Output layer will have vocab_size number of neurons\n",
    "# Number of weights per neuron will be equal to n_embed * block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7eebbc06-6b55-4843-a269-a3b101edf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the embeddings \n",
    "n_embed = 50\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "\n",
    "n_hidden_1 = 200\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden_1), generator=g) * 1 /  (n_embed * block_size)**0.5\n",
    "B1 = torch.randn((1, n_hidden_1), generator=g) * 0.1\n",
    "\n",
    "n_hidden_2 = 200 \n",
    "W2 = torch.randn((n_hidden_1, n_hidden_2), generator=g) * 1 / (n_hidden_1 ** 0.5)\n",
    "B2 = torch.randn((1, n_hidden_2), generator=g) * 0\n",
    "\n",
    "n_hidden_3 = 200 \n",
    "W3 = torch.randn((n_hidden_2, n_hidden_3), generator=g) * 1 / (n_hidden_1 ** 0.5)\n",
    "B3 = torch.randn((1, n_hidden_3), generator=g) * 0\n",
    "\n",
    "# Output layer \n",
    "n_output = vocab_size\n",
    "W4 = torch.randn((n_hidden_3, n_output), generator=g) *  1 / (n_hidden_2 ** 0.5)\n",
    "B4 = torch.randn((1, n_output), generator=g)\n",
    "\n",
    "parameters = [W1, B1, W2, B2, W3, B3, W4, B4]\n",
    "\n",
    "\n",
    "# Let's set requires_grad to True for all parameters\n",
    "for p in parameters: \n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6718c09c-a6c8-4ba4-afad-9ff7ac6e4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at: 0: 10.991805076599121\n",
      "loss at: 100: 8.808565139770508\n",
      "loss at: 200: 8.181946754455566\n",
      "loss at: 300: 7.795853137969971\n",
      "loss at: 400: 7.579355239868164\n",
      "loss at: 500: 7.590691089630127\n",
      "loss at: 600: 7.651228427886963\n",
      "loss at: 700: 7.4881415367126465\n",
      "loss at: 800: 7.689975738525391\n",
      "loss at: 900: 7.489752769470215\n",
      "loss at: 1000: 7.483675003051758\n",
      "loss at: 1100: 7.383844375610352\n",
      "loss at: 1200: 7.465594291687012\n",
      "loss at: 1300: 7.448939323425293\n",
      "loss at: 1400: 7.558730602264404\n",
      "loss at: 1500: 7.315492630004883\n",
      "loss at: 1600: 7.269989490509033\n",
      "loss at: 1700: 7.366788864135742\n",
      "loss at: 1800: 7.061119556427002\n",
      "loss at: 1900: 7.210980415344238\n",
      "loss at: 2000: 7.247604846954346\n",
      "loss at: 2100: 7.379008769989014\n",
      "loss at: 2200: 7.080563068389893\n",
      "loss at: 2300: 7.298737525939941\n",
      "loss at: 2400: 7.426915168762207\n",
      "loss at: 2500: 7.107969284057617\n",
      "loss at: 2600: 7.3432536125183105\n",
      "loss at: 2700: 7.238056659698486\n",
      "loss at: 2800: 7.119639873504639\n",
      "loss at: 2900: 7.217613697052002\n",
      "loss at: 3000: 7.086688041687012\n",
      "loss at: 3100: 7.069308280944824\n",
      "loss at: 3200: 7.11886739730835\n",
      "loss at: 3300: 7.1182637214660645\n",
      "loss at: 3400: 7.206608772277832\n",
      "loss at: 3500: 7.260257720947266\n",
      "loss at: 3600: 7.256787300109863\n",
      "loss at: 3700: 7.053725242614746\n",
      "loss at: 3800: 7.065742015838623\n",
      "loss at: 3900: 6.890349864959717\n",
      "loss at: 4000: 7.124217510223389\n",
      "loss at: 4100: 6.982151985168457\n",
      "loss at: 4200: 7.1095075607299805\n",
      "loss at: 4300: 6.845959663391113\n",
      "loss at: 4400: 7.075541973114014\n",
      "loss at: 4500: 6.972281455993652\n",
      "loss at: 4600: 7.093386650085449\n",
      "loss at: 4700: 7.05930757522583\n",
      "loss at: 4800: 6.82509708404541\n",
      "loss at: 4900: 7.012200355529785\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "epoch = 5000\n",
    "for i in range(epoch):\n",
    "    # forward pass\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size, ))\n",
    "    emb = C[Xtr[ix]]\n",
    "    embeddings = emb.flatten(start_dim=1, end_dim=-1)\n",
    "\n",
    "    hpreact_1 = embeddings @ W1 + B1\n",
    "    h_1 = torch.tanh(hpreact_1)\n",
    "\n",
    "    hpreact_2 = h_1 @ W2  + B2 \n",
    "    h_2 = torch.tanh(hpreact_2)\n",
    "\n",
    "    hpreact_3 = h_2 @ W3  + B3 \n",
    "    h_3 = torch.tanh(hpreact_3)\n",
    "\n",
    "    logits = h_3 @ W4 + B4 \n",
    "\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    \n",
    "    # Backward pass\n",
    "    # update the grad to None\n",
    "    for p in parameters: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    e = 0.1 if i < 500 else 0.05\n",
    "    \n",
    "    for p in parameters: \n",
    "        p.data -= e * p.grad\n",
    "    if i % 100 == 0:\n",
    "        print(f\"loss at: {i}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f94f9dd9-f5f2-4cd3-ab8c-0e95ec67ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(split):\n",
    "    # Select data based on split\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'validation': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    # Forward pass through the model\n",
    "    emb = C[x]\n",
    "    embeddings = emb.flatten(start_dim=1, end_dim=-1)\n",
    "    \n",
    "    # Hidden layer 1\n",
    "    hpreact_1 = embeddings @ W1 + B1\n",
    "    h_1 = torch.tanh(hpreact_1)\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    hpreact_2 = h_1 @ W2 + B2\n",
    "    h_2 = torch.tanh(hpreact_2)\n",
    "    \n",
    "    # Output layer (logits)\n",
    "    logits = h_2 @ W3 + B3\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9c90dc0-3ee1-44d0-b268-a24476c68886",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(f\"Train loss: {evaluate_loss('train')}\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mevaluate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 26\u001b[0m, in \u001b[0;36mevaluate_loss\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m h_2 \u001b[38;5;241m@\u001b[39m W3 \u001b[38;5;241m+\u001b[39m B3\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/functional.py:2969\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2964\u001b[0m         reduced \u001b[38;5;241m=\u001b[39m reduced \u001b[38;5;241m/\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduced\n\u001b[0;32m-> 2969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy\u001b[39m(\n\u001b[1;32m   2970\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m   2971\u001b[0m     target: Tensor,\n\u001b[1;32m   2972\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2973\u001b[0m     size_average: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2974\u001b[0m     ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   2975\u001b[0m     reduce: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2976\u001b[0m     reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2977\u001b[0m     label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   2978\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[1;32m   2980\u001b[0m \n\u001b[1;32m   2981\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.CrossEntropyLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;124;03m        >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(f\"Train loss: {evaluate_loss('train')}\")\n",
    "print(f\"Validation loss: {evaluate_loss('validation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8c2c0f8-9a18-4b29-8a9c-44508586e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . abscissa abolition abated abernathys abyss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_sentence(start_sequence, max_length=10, period_idx=None):\n",
    "    # Ensure the start sequence has exactly 5 tokens\n",
    "    assert len(start_sequence) == 5, \"The start sequence must be of length 5\"\n",
    "    \n",
    "    # Initialize the generated sentence with the start sequence\n",
    "    generated_sentence = start_sequence[:]\n",
    "\n",
    "    # Generate words until max_length is reached\n",
    "    for _ in range(max_length - len(start_sequence)):\n",
    "        # Prepare input by embedding the current sequence\n",
    "        input_tensor = torch.tensor(generated_sentence[-5:]).unsqueeze(0)  # Only use the last 5 tokens\n",
    "        emb = C[input_tensor]\n",
    "        embeddings = emb.flatten(start_dim=1, end_dim=-1)  # Flatten to match input shape\n",
    "\n",
    "        # Forward pass through the model\n",
    "        hpreact_1 = embeddings @ W1 + B1\n",
    "        h_1 = torch.tanh(hpreact_1)\n",
    "        \n",
    "        hpreact_2 = h_1 @ W2 + B2\n",
    "        h_2 = torch.tanh(hpreact_2)\n",
    "        \n",
    "        logits = h_2 @ W3 + B3\n",
    "\n",
    "        # Get probabilities for the next word\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample the next word from the probability distribution\n",
    "        next_word_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Append the next word to the generated sentence\n",
    "        generated_sentence.append(next_word_idx)\n",
    "\n",
    "        # Break if the generated word is a period\n",
    "        if next_word_idx == period_idx:\n",
    "            break\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "# Example usage\n",
    "# Assuming `start_sequence` is a list of 5 starting word indices and `period_idx` is the index of \".\"\n",
    "start_sequence = [0, 0, 0, 0, 0]  # Replace with actual indices from your vocabulary\n",
    "period_idx = 0  # Replace with the actual index of \".\" in your vocabulary\n",
    "\n",
    "generated_sentence_indices = generate_sentence(start_sequence, period_idx=period_idx)\n",
    "\n",
    "# Convert indices back to words if you have a `vocab` list or dictionary\n",
    "print(' '.join(itos[i] for i in generated_sentence_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "701d8fe7-6b24-4534-b963-5fb587a9589c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[torch.tensor([0, 0, 0, 0, 0])]\n",
    "embeddings.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e7c8-c8b7-47f5-be64-2820518da935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945f8f1-ac0d-4d83-a223-3fe1cdc10fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
