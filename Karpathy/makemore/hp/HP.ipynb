{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a137b0c-500b-4079-bcb2-1f8b0d8d35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/pdixit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "803c95c7-e406-4482-9d76-6d38639a05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed63cbde-2991-444c-9b98-97b8a4590107",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Book1.txt\", encoding=\"utf8\") as f:\n",
    "    book = f.read()\n",
    "    book_pages = re.sub(\"Page.*?Rowling\", \"\", book, flags=re.DOTALL)\n",
    "    book_chapters = re.sub(\"(?:\\\\n){5,}[A-Z- \\\\n]+ (?:\\\\n){2,}\", \"\", book_pages)\n",
    "    book_line_breaks = re.sub(\"\\\\n\", \"\", book_chapters)\n",
    "    sentence_split = sent_tokenize(book_chapters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d352b6ee-60c9-4373-9c9e-68b8f58a7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = [[word.lower() for word in word_tokenize(sentence)] for sentence in sentence_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe296d6f-64dd-44bd-9dda-d198410f5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set() \n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    for word in sentence: \n",
    "        vocab.add(word)\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90af1c32-7639-4cc6-bc5e-bd99ddaa846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "120f39b7-9902-4849-a724-2510678443bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b75c2de3-cc39-46b7-8f1f-36c86d5dd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7828e3ac-c9f0-4309-b92e-9b7eac7e6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sentences\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99374c3a-7b7e-4c6e-b861-d9c82e0eee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 15 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "Xtr = Xtr.to(device)\n",
    "Ytr = Ytr.to(device)\n",
    "\n",
    "Xdev = Xdev.to(device)\n",
    "Ydev = Ydev.to(device)\n",
    "\n",
    "Xte = Xte.to(device)\n",
    "Yte = Yte.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b39291ae-420d-4656-ac02-e4936fe5c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81153, 15]) torch.Size([81153])\n",
      "torch.Size([10082, 15]) torch.Size([10082])\n",
      "torch.Size([9934, 15]) torch.Size([9934])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 15 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(sentences):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    context = [stoi['.']] * block_size\n",
    "    for word in sentence:\n",
    "      ix = stoi[word]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "      \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(processed_sentences))\n",
    "n2 = int(0.9*len(processed_sentences))\n",
    "Xtr,  Ytr  = build_dataset(processed_sentences[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(processed_sentences[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(processed_sentences[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4a921fe4-0923-49f9-b1ee-db5fade8b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), device=device) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    \n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # Parameters (trainable via backprop)\n",
    "    self.gamma = torch.ones(dim, device=device).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    self.beta = torch.zeros(dim, device=device).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    # Buffers (updated via momentum)\n",
    "    self.running_mean = torch.zeros(dim, device=device)  # Shape [C]\n",
    "    self.running_var = torch.ones(dim, device=device)    # Shape [C]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if self.training:\n",
    "      # Compute mean and variance across batch and sequence length (dim=(0,2))\n",
    "      xmean = x.mean(dim=(0, 2), keepdim=True)  # Shape [1, C, 1]\n",
    "      xvar = x.var(dim=(0, 2), keepdim=True)    # Shape [1, C, 1]\n",
    "    else:\n",
    "      # Use running statistics for inference\n",
    "      xmean = self.running_mean.view(1, -1, 1)  # Shape [1, C, 1]\n",
    "      xvar = self.running_var.view(1, -1, 1)    # Shape [1, C, 1]\n",
    "    \n",
    "    # Normalize input\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta         # Scale and shift\n",
    "\n",
    "    # Update running statistics during training\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze()\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze()\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Return trainable parameters\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim), device=device)\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX].transpose(1, 2)\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# --------------------------------------------\n",
    "class Conv1d:\n",
    "    def __init__(self, sequence_length, in_channels, out_channels, kernel=2, stride=1, dilation=1):\n",
    "        self. sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel), device=device) * ((2 / (in_channels * kernel)) ** 0.5)\n",
    "        self.bias = torch.randn(out_channels, device=device) * 0\n",
    "        self.effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        self.Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "    def __call__(self, x):\n",
    "        # Compute effective kernel size based on dilation \n",
    "        # effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        \n",
    "        N, C, L = x.shape\n",
    "        assert self.effective_kernel <= L\n",
    "            \n",
    "        # create the sliding windows of the input \n",
    "        x_unfolded = x.unfold(2, self.effective_kernel, self.stride)\n",
    "\n",
    "        # Extract dilated inputs from x_unfolded which used effective_kernel. The shape of the unfolded vector is [N, C, L, effective_k] \n",
    "        # where L is the length of the sequence depending on the effective kernel. From the dimension of effective_kernel, we clip every 'dilated' index\n",
    "        # If effective_kernel is 3 and dilation is 2, [1, 2, 3] will result in [1, 3]. [1,3] has length of 2, which is equal to actual kernel value\n",
    "        x_unfolded = x_unfolded[:, :, :, ::self.dilation]\n",
    "\n",
    "        # The dilation also changes the sequence length, since effective kernel value changes with dilation > 1. \n",
    "        # Compute Lout based on effective_kernel\n",
    "        \n",
    "        # Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "        \n",
    "        # Before cross correlation, we need to broadcast the filters and the input correctly\n",
    "        x_unfolded = x_unfolded.view(N, 1, C, self.Lout, self.kernel)\n",
    "        filters = self.filters.view(1, self.out_channels, self.in_channels, 1, self.kernel)\n",
    "\n",
    "        # Perform element wise multiplication\n",
    "        self.out = torch.mul(x_unfolded, filters).sum((2, 4)) + self.bias.view(1, self.out_channels, 1)\n",
    "        return self.out        \n",
    "    \n",
    "    def parameters(self): \n",
    "        return [self.filters] + [self.bias]\n",
    "\n",
    "class ReLu: \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "class Transpose:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.transpose(1, 2)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Residual: \n",
    "    def __init__(self, layers):\n",
    "        self.projection_needed = False\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "        # Input of the in layer config\n",
    "        in_layer = layers[0]\n",
    "        out_layer = layers[-1]\n",
    "\n",
    "        self.in_channels = in_layer.in_channels\n",
    "        self.in_sequence_length = in_layer.sequence_length\n",
    "\n",
    "        self.out_channels = out_layer.out_channels\n",
    "        self.out_sequence_length = out_layer.Lout\n",
    "\n",
    "        if self.in_channels != self.out_channels: # Assuming for now, this will always be the case\n",
    "            self.projection_needed = True\n",
    "            self.linear_projection_conv = Conv1d(sequence_length=self.in_sequence_length, out_channels=self.out_channels, in_channels=self.in_channels, kernel=1)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        # Perform residual operation\n",
    "        if self.projection_needed:\n",
    "            linear_projection = self.linear_projection_conv(self.input)\n",
    "            \n",
    "        # Pad the output since the Lout != Lin\n",
    "        sequence_length_diff = self.in_sequence_length - self.out_sequence_length\n",
    "        x = F.pad(x, (sequence_length_diff // 2, sequence_length_diff - sequence_length_diff // 2))\n",
    "        \n",
    "        self.out = x + linear_projection if self.projection_needed else x + self.input\n",
    "        \n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        # Collect parameters from all layers and the projection (if used)\n",
    "        params = [p for layer in self.layers for p in layer.parameters()]\n",
    "        if self.projection_needed:\n",
    "            params += self.linear_projection_conv.parameters()\n",
    "        return params\n",
    "\n",
    "class ElmanRNN:\n",
    "    def __init__(self, input_channels, hidden_channels): # Bias would be present. Tanh non linearity will be applied. \n",
    "        self.wxh = torch.randn((input_channels, hidden_channels), device=device) * (5/3 / (input_channels) ** 0.5)\n",
    "        self.bxh = torch.randn((1, hidden_channels), device=device) * 0.001\n",
    "        \n",
    "        self.whh = torch.randn((hidden_channels, hidden_channels), device=device) * (5/3 / (hidden_channels) ** 0.5)\n",
    "        self.bhh = torch.randn((1, hidden_channels), device=device) * 0.001\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def __call__(self, x):\n",
    "        N, C, L = x.shape\n",
    "\n",
    "        # Iterate over the entire sequence length and generate logits. The shape of the output logits will be [N, Hout, L]\n",
    "        logits = torch.zeros((N, self.hidden_channels, L), device=device)\n",
    "        H = torch.zeros((N, self.hidden_channels), device=device)\n",
    "        for i in range(L):\n",
    "            xi = x[:, :, i]\n",
    "            xih = (xi @ self.wxh) + self.bxh\n",
    "            hh = (H @ self.whh) + self.bhh\n",
    "            ht = torch.tanh(xih + hh)\n",
    "            \n",
    "            H = ht \n",
    "            logits[:, :, i] = ht\n",
    "            self.out = logits\n",
    "            \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.wxh] + [self.whh] + [self.bxh] + [self.bhh]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2c0b3ed8-50dd-45a3-bbc4-f61bc6c1e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 11072854\n"
     ]
    }
   ],
   "source": [
    "n_embeddings = 240\n",
    "h_channels = 100\n",
    "h2_channels = 100\n",
    "h3_channels = 100\n",
    "h4_channels = 100\n",
    "h5_channels = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embeddings),\n",
    "    ElmanRNN(n_embeddings, h_channels),\n",
    "    ElmanRNN(h_channels, h2_channels),\n",
    "    ElmanRNN(h2_channels, h3_channels),\n",
    "    ElmanRNN(h3_channels, h4_channels),\n",
    "    ElmanRNN(h4_channels, h5_channels),\n",
    "\n",
    "\n",
    "    Flatten(), Linear(h5_channels * block_size, vocab_size)\n",
    "])\n",
    "# parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"parameters: {sum(p.nelement() for p in model.parameters())}\")\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a77b3027-4286-428d-86fc-4a8ce4a5e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "eeb6cf04-4a25-401c-9900-2642134e79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 9.1723\n",
      "   1000/ 100000: 8.2699\n",
      "   2000/ 100000: 7.8623\n",
      "   3000/ 100000: 8.0676\n",
      "   4000/ 100000: 7.7469\n",
      "   5000/ 100000: 7.1291\n",
      "   6000/ 100000: 7.2876\n",
      "   7000/ 100000: 7.2230\n",
      "   8000/ 100000: 7.4419\n",
      "   9000/ 100000: 6.6004\n",
      "  10000/ 100000: 6.7959\n",
      "  11000/ 100000: 6.5824\n",
      "  12000/ 100000: 5.9651\n",
      "  13000/ 100000: 6.2126\n",
      "  14000/ 100000: 6.8038\n",
      "  15000/ 100000: 5.6083\n",
      "  16000/ 100000: 6.0809\n",
      "  17000/ 100000: 5.9201\n",
      "  18000/ 100000: 6.6692\n",
      "  19000/ 100000: 5.2723\n",
      "  20000/ 100000: 6.3483\n",
      "  21000/ 100000: 5.9867\n",
      "  22000/ 100000: 6.4864\n",
      "  23000/ 100000: 5.8675\n",
      "  24000/ 100000: 5.4410\n",
      "  25000/ 100000: 5.9410\n",
      "  26000/ 100000: 6.0777\n",
      "  27000/ 100000: 5.8142\n",
      "  28000/ 100000: 5.8624\n",
      "  29000/ 100000: 5.8944\n",
      "  30000/ 100000: 6.4390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[241], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     13\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Update the parameters \u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time_ns()\n",
    "max_steps = 100000\n",
    "for i in range(max_steps): \n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "    \n",
    "    # Forward pass \n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward \n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "    \n",
    "    # Update the parameters \n",
    "    e = 0.01  if i < 20_000 else 0.005\n",
    "    for p in model.parameters():\n",
    "        p.data += -e * p.grad\n",
    "    \n",
    "    if i % 1000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "end_time = time.time_ns() \n",
    "print(f\"Total time: {(end_time - start_time) / 1_000_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7832c-b227-492c-8ddb-273c503dc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  \n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be3dca-4df6-48c1-a3dd-bd4a6dfe624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "94136286-da35-45f5-b5fd-52bcb3860d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model():\n",
    "    res = []\n",
    "    for _ in range(2):\n",
    "        out = []\n",
    "        context = [stoi['.']] * block_size # initialize with all ...\n",
    "        context[block_size-2] = stoi['hogwarts']\n",
    "        context[block_size-1] = stoi['castle']\n",
    "        while True:\n",
    "          # forward pass the neural net\n",
    "          logits = model(torch.tensor([context]))\n",
    "          probs = F.softmax(logits, dim=1)\n",
    "          # sample from the distribution\n",
    "          ix = torch.multinomial(probs, num_samples=1).item()\n",
    "          # shift the context window and track the samples\n",
    "          context = context[1:] + [ix]\n",
    "          out.append(ix)\n",
    "          # if we sample the special '.' token, break\n",
    "          if ix == 0:\n",
    "            break\n",
    "        res.append(' '.join(itos[i] for i in out)) # decode and print the generated word\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4b62ab27-605b-48b2-ad24-6d4fdd6f984a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['; what hagrid around as the know-who of rapidly ronan goat . a match the dropped stay gloomy bacon — and “ into !',\n",
       " '’ s about send ? ” “ riders — i ’ ve got history. here to parcel ’ t that an troll a sound !']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "55ad12f8-50f0-4376-9322-b37b927338c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/84/r3s7dwr93cs3k0kzl447b_8c0000gp/T/ipykernel_11155/1295192912.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lossi = torch.tensor(lossi)\n"
     ]
    }
   ],
   "source": [
    "lossi = torch.tensor(lossi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d03a41a2-c5b0-47db-a1ec-04c5ae9bc30a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 62110",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlossi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 62110"
     ]
    }
   ],
   "source": [
    "plt.plot(lossi.view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d3b6d-f439-407a-937b-e1479e639ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
