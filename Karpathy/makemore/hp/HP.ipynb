{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a137b0c-500b-4079-bcb2-1f8b0d8d35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/prakhardixit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "803c95c7-e406-4482-9d76-6d38639a05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed63cbde-2991-444c-9b98-97b8a4590107",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"Book1.txt\", encoding=\"utf8\") as f:\n",
    "    book = f.read()\n",
    "    book_pages = re.sub(\"Page.*?Rowling\", \"\", book, flags=re.DOTALL)\n",
    "    book_chapters = re.sub(\"(?:\\\\n){5,}[A-Z- \\\\n]+ (?:\\\\n){2,}\", \"\", book_pages)\n",
    "    book_line_breaks = re.sub(\"\\\\n\", \"\", book_chapters)\n",
    "    sentence_split = sent_tokenize(book_chapters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d352b6ee-60c9-4373-9c9e-68b8f58a7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = [[word.lower() for word in word_tokenize(sentence)] for sentence in sentence_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe296d6f-64dd-44bd-9dda-d198410f5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set() \n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    for word in sentence: \n",
    "        vocab.add(word)\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90af1c32-7639-4cc6-bc5e-bd99ddaa846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stoi = {s:i for i,s in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "120f39b7-9902-4849-a724-2510678443bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b75c2de3-cc39-46b7-8f1f-36c86d5dd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7828e3ac-c9f0-4309-b92e-9b7eac7e6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sentences\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99374c3a-7b7e-4c6e-b861-d9c82e0eee4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, Y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n\u001b[0;32m---> 20\u001b[0m n1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mwords\u001b[49m))\n\u001b[1;32m     21\u001b[0m n2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n\u001b[1;32m     22\u001b[0m Xtr,  Ytr  \u001b[38;5;241m=\u001b[39m build_dataset(words[:n1])     \u001b[38;5;66;03m# 80%\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 15 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "Xtr = Xtr.to(device)\n",
    "Ytr = Ytr.to(device)\n",
    "\n",
    "Xdev = Xdev.to(device)\n",
    "Ydev = Ydev.to(device)\n",
    "\n",
    "Xte = Xte.to(device)\n",
    "Yte = Yte.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b39291ae-420d-4656-ac02-e4936fe5c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81153, 15]) torch.Size([81153])\n",
      "torch.Size([10082, 15]) torch.Size([10082])\n",
      "torch.Size([9934, 15]) torch.Size([9934])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 15 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(sentences):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    context = [stoi['.']] * block_size\n",
    "    for word in sentence:\n",
    "      ix = stoi[word]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "      \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(processed_sentences))\n",
    "n2 = int(0.9*len(processed_sentences))\n",
    "Xtr,  Ytr  = build_dataset(processed_sentences[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(processed_sentences[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(processed_sentences[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a921fe4-0923-49f9-b1ee-db5fade8b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), device=device) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    \n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # Parameters (trainable via backprop)\n",
    "    self.gamma = torch.ones(dim, device=device).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    self.beta = torch.zeros(dim, device=device).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    # Buffers (updated via momentum)\n",
    "    self.running_mean = torch.zeros(dim, device=device)  # Shape [C]\n",
    "    self.running_var = torch.ones(dim, device=device)    # Shape [C]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if self.training:\n",
    "      # Compute mean and variance across batch and sequence length (dim=(0,2))\n",
    "      xmean = x.mean(dim=(0, 2), keepdim=True)  # Shape [1, C, 1]\n",
    "      xvar = x.var(dim=(0, 2), keepdim=True)    # Shape [1, C, 1]\n",
    "    else:\n",
    "      # Use running statistics for inference\n",
    "      xmean = self.running_mean.view(1, -1, 1)  # Shape [1, C, 1]\n",
    "      xvar = self.running_var.view(1, -1, 1)    # Shape [1, C, 1]\n",
    "    \n",
    "    # Normalize input\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta         # Scale and shift\n",
    "\n",
    "    # Update running statistics during training\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze()\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze()\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Return trainable parameters\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim), device=device)\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX].transpose(1, 2)\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# --------------------------------------------\n",
    "class Conv1d:\n",
    "    def __init__(self, sequence_length, in_channels, out_channels, kernel=2, stride=1, dilation=1):\n",
    "        self. sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel), device=device) * ((2 / (in_channels * kernel)) ** 0.5)\n",
    "        self.bias = torch.randn(out_channels, device=device) * 0\n",
    "        self.effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        self.Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "    def __call__(self, x):\n",
    "        # Compute effective kernel size based on dilation \n",
    "        # effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        \n",
    "        N, C, L = x.shape\n",
    "        assert self.effective_kernel <= L\n",
    "            \n",
    "        # create the sliding windows of the input \n",
    "        x_unfolded = x.unfold(2, self.effective_kernel, self.stride)\n",
    "\n",
    "        # Extract dilated inputs from x_unfolded which used effective_kernel. The shape of the unfolded vector is [N, C, L, effective_k] \n",
    "        # where L is the length of the sequence depending on the effective kernel. From the dimension of effective_kernel, we clip every 'dilated' index\n",
    "        # If effective_kernel is 3 and dilation is 2, [1, 2, 3] will result in [1, 3]. [1,3] has length of 2, which is equal to actual kernel value\n",
    "        x_unfolded = x_unfolded[:, :, :, ::self.dilation]\n",
    "\n",
    "        # The dilation also changes the sequence length, since effective kernel value changes with dilation > 1. \n",
    "        # Compute Lout based on effective_kernel\n",
    "        \n",
    "        # Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "        \n",
    "        # Before cross correlation, we need to broadcast the filters and the input correctly\n",
    "        x_unfolded = x_unfolded.view(N, 1, C, self.Lout, self.kernel)\n",
    "        filters = self.filters.view(1, self.out_channels, self.in_channels, 1, self.kernel)\n",
    "\n",
    "        # Perform element wise multiplication\n",
    "        self.out = torch.mul(x_unfolded, filters).sum((2, 4)) + self.bias.view(1, self.out_channels, 1)\n",
    "        return self.out        \n",
    "    \n",
    "    def parameters(self): \n",
    "        return [self.filters] + [self.bias]\n",
    "\n",
    "class ReLu: \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "class Transpose:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.transpose(1, 2)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Residual: \n",
    "    def __init__(self, layers):\n",
    "        self.projection_needed = False\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "        # Input of the in layer config\n",
    "        in_layer = layers[0]\n",
    "        out_layer = layers[-1]\n",
    "\n",
    "        self.in_channels = in_layer.in_channels\n",
    "        self.in_sequence_length = in_layer.sequence_length\n",
    "\n",
    "        self.out_channels = out_layer.out_channels\n",
    "        self.out_sequence_length = out_layer.Lout\n",
    "\n",
    "        if self.in_channels != self.out_channels: # Assuming for now, this will always be the case\n",
    "            self.projection_needed = True\n",
    "            self.linear_projection_conv = Conv1d(sequence_length=self.in_sequence_length, out_channels=self.out_channels, in_channels=self.in_channels, kernel=1)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "        # Perform residual operation\n",
    "        if self.projection_needed:\n",
    "            linear_projection = self.linear_projection_conv(self.input)\n",
    "            \n",
    "        # Pad the output since the Lout != Lin\n",
    "        sequence_length_diff = self.in_sequence_length - self.out_sequence_length\n",
    "        x = F.pad(x, (sequence_length_diff // 2, sequence_length_diff - sequence_length_diff // 2))\n",
    "        \n",
    "        self.out = x + linear_projection if self.projection_needed else x + self.input\n",
    "        \n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        # Collect parameters from all layers and the projection (if used)\n",
    "        params = [p for layer in self.layers for p in layer.parameters()]\n",
    "        if self.projection_needed:\n",
    "            params += self.linear_projection_conv.parameters()\n",
    "        return params\n",
    "\n",
    "class ElmanRNN:\n",
    "    def __init__(self, input_channels, hidden_channels): # Bias would be present. Tanh non linearity will be applied. \n",
    "        self.wxh = torch.randn((input_channels, hidden_channels), device=device) * (5/3 / (input_channels) ** 0.5)\n",
    "        self.bxh = torch.randn((1, hidden_channels), device=device) * 0.001\n",
    "        \n",
    "        self.whh = torch.randn((hidden_channels, hidden_channels), device=device) * (5/3 / (hidden_channels) ** 0.5)\n",
    "        self.bhh = torch.randn((1, hidden_channels), device=device) * 0.001\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.input_channels = input_channels\n",
    "\n",
    "    def __call__(self, x):\n",
    "        N, C, L = x.shape\n",
    "\n",
    "        # Iterate over the entire sequence length and generate logits. The shape of the output logits will be [N, Hout, L]\n",
    "        logits = torch.zeros((N, self.hidden_channels, L), device=device)\n",
    "        H = torch.zeros((N, self.hidden_channels), device=device)\n",
    "        for i in range(L):\n",
    "            xi = x[:, :, i]\n",
    "            xih = (xi @ self.wxh) + self.bxh\n",
    "            hh = (H @ self.whh) + self.bhh\n",
    "            ht = torch.tanh(xih + hh)\n",
    "            \n",
    "            H = ht \n",
    "            logits[:, :, i] = ht\n",
    "            self.out = logits\n",
    "            \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.wxh] + [self.whh] + [self.bxh] + [self.bhh]\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define gate weights \n",
    "        # Input Gate weights\n",
    "        self. wii = torch.randn((input_size, hidden_size)) / (input_size ** 0.5)\n",
    "        self.whi = torch.randn((hidden_size, hidden_size)) / (hidden_size ** 0.5)\n",
    "        self.bii = torch.randn(hidden_size) * 0.001\n",
    "        \n",
    "        # Forget Gate weights \n",
    "        self.wif = torch.randn((input_size, hidden_size)) / (input_size ** 0.5)\n",
    "        self.whf = torch.randn((hidden_size, hidden_size)) / (input_size ** 0.5)\n",
    "        self.bif = torch.randn(hidden_size) * 0.001\n",
    "\n",
    "        # Cell candidiate weights \n",
    "        self.wig = torch.randn((input_size, hidden_size)) * ((5/3) / (input_size ** 0.5))\n",
    "        self.whg = torch.randn((hidden_size, hidden_size)) * ((5/3) / (input_size ** 0.5))\n",
    "        self.big = torch.randn(hidden_size) * 0.001\n",
    "        # Output gate weights \n",
    "        self.wio = torch.randn((input_size, hidden_size)) / (input_size ** 0.5)\n",
    "        self.who = torch.randn((hidden_size, hidden_size)) / (hidden_size ** 0.5)\n",
    "        self.bio = torch.randn(hidden_size) * 0.001\n",
    "\n",
    "    def __call__(self, x, H=None, C=None):\n",
    "        # Shape of x will be [N, C, L]. N = batch size, C = input channel size, L = input sequence length \n",
    "        N, I, L = x.shape \n",
    "        if H == None:\n",
    "            H = torch.zeros((N, self.hidden_size))\n",
    "        if C == None: \n",
    "            C = torch.zeros((N, self.hidden_size))\n",
    "        logits = torch.zeros((N, self.hidden_size, L))\n",
    "        for i in range(x.shape[2]):\n",
    "            xi = x[:, :, i]\n",
    "            # Compute gate vectors \n",
    "            it = torch.sigmoid((xi @ self.wii) + (H @ self.whi) + self.bii)\n",
    "            ft = torch.sigmoid((xi @ self.wif) + (H @ self.whf) + self.bif)\n",
    "            gt = torch.tanh((xi @ self.wig) + (H @ self.whg) + self.big)\n",
    "            ot = torch.sigmoid((xi @ self.wio) + (H @ self.who) + self.bio)\n",
    "            ct = (ft * C) + (it * gt)\n",
    "            C = ct\n",
    "            ht = ot * torch.tanh(ct)\n",
    "            H = ht\n",
    "            logits[:, :, i] = ht\n",
    "        self.out = logits\n",
    "        self.C = C\n",
    "        return self.out, self.C\n",
    "    def parameters(self):\n",
    "        return [self.wii, self.whi, self.bii] + [self.wif, self.whf, self.bif] + [self.wig, self.whg, self.big] + [self.wio, self.who, self.bio]\n",
    "\n",
    "class LayeredLSTM:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "    def __call__(self, x, h=None, c=None):\n",
    "        N, I, L = x.shape  # N=batch size, I=input channels, L=sequence length\n",
    "\n",
    "        if h is None:\n",
    "            h = [torch.zeros((N, layer.hidden_size)) for layer in self.layers]\n",
    "        if c is None:\n",
    "            c = [torch.zeros((N, layer.hidden_size)) for layer in self.layers]\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, cell_state = layer(x, h[i], c[i])\n",
    "            h[i] = x[:, :, -1]\n",
    "            c[i] = cell_state\n",
    "        \n",
    "        return x\n",
    "    def parameters(self): \n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c0b3ed8-50dd-45a3-bbc4-f61bc6c1e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: 60213854\n"
     ]
    }
   ],
   "source": [
    "n_embeddings = 240\n",
    "h_channels = 500\n",
    "h2_channels = 500\n",
    "h3_channels = 500\n",
    "h4_channels = 500\n",
    "h5_channels = 500\n",
    "h6_channels = 500\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embeddings),\n",
    "    LayeredLSTM([\n",
    "        LSTM(n_embeddings, h_channels),\n",
    "        LSTM(h_channels, h2_channels),\n",
    "        LSTM(h2_channels, h3_channels),\n",
    "        LSTM(h3_channels, h4_channels),\n",
    "        LSTM(h4_channels, h5_channels),\n",
    "        LSTM(h5_channels, h6_channels),\n",
    "    ]),\n",
    "    \n",
    "    Flatten(), Linear(h6_channels * block_size, vocab_size)\n",
    "])\n",
    "# parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"parameters: {sum(p.nelement() for p in model.parameters())}\")\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a77b3027-4286-428d-86fc-4a8ce4a5e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eeb6cf04-4a25-401c-9900-2642134e79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 8.7507\n",
      "   1000/ 100000: 8.7397\n",
      "   2000/ 100000: 8.7217\n",
      "   3000/ 100000: 8.7001\n",
      "   4000/ 100000: 8.6991\n",
      "   5000/ 100000: 8.6550\n",
      "   6000/ 100000: 8.6541\n",
      "   7000/ 100000: 8.6397\n",
      "   8000/ 100000: 8.6233\n",
      "   9000/ 100000: 8.5614\n",
      "  10000/ 100000: 8.5749\n",
      "  11000/ 100000: 8.5202\n",
      "  12000/ 100000: 8.5025\n",
      "  13000/ 100000: 8.4437\n",
      "  14000/ 100000: 8.3735\n",
      "  15000/ 100000: 8.3574\n",
      "  16000/ 100000: 8.4053\n",
      "  17000/ 100000: 8.2237\n",
      "  18000/ 100000: 8.3682\n",
      "  19000/ 100000: 7.9079\n",
      "  20000/ 100000: 8.2673\n",
      "  21000/ 100000: 8.1530\n",
      "  22000/ 100000: 7.8671\n",
      "  23000/ 100000: 8.0652\n",
      "  24000/ 100000: 8.0215\n",
      "  25000/ 100000: 7.8828\n",
      "  26000/ 100000: 8.0973\n",
      "  27000/ 100000: 7.9143\n",
      "  28000/ 100000: 7.9313\n",
      "  29000/ 100000: 7.8515\n",
      "  30000/ 100000: 7.7310\n",
      "  31000/ 100000: 7.7024\n",
      "  32000/ 100000: 7.7440\n",
      "  33000/ 100000: 7.3868\n",
      "  34000/ 100000: 7.7073\n",
      "  35000/ 100000: 7.5459\n",
      "  36000/ 100000: 7.4076\n",
      "  37000/ 100000: 7.8450\n",
      "  38000/ 100000: 7.2715\n",
      "  39000/ 100000: 7.3101\n",
      "  40000/ 100000: 7.2034\n",
      "  41000/ 100000: 7.4849\n",
      "  42000/ 100000: 7.4348\n",
      "  43000/ 100000: 7.3646\n",
      "  44000/ 100000: 7.3742\n",
      "  45000/ 100000: 7.2642\n",
      "  46000/ 100000: 7.2388\n",
      "  47000/ 100000: 7.1559\n",
      "  48000/ 100000: 6.9741\n",
      "  49000/ 100000: 7.1208\n",
      "  50000/ 100000: 6.9343\n",
      "  51000/ 100000: 7.0331\n",
      "  52000/ 100000: 7.0346\n",
      "  53000/ 100000: 7.0787\n",
      "  54000/ 100000: 7.2474\n",
      "  55000/ 100000: 6.8039\n",
      "  56000/ 100000: 6.5435\n",
      "  57000/ 100000: 7.0259\n",
      "  58000/ 100000: 6.8641\n",
      "  59000/ 100000: 7.0078\n",
      "  60000/ 100000: 6.9040\n",
      "  61000/ 100000: 6.9548\n",
      "  62000/ 100000: 6.6432\n",
      "  63000/ 100000: 7.1766\n",
      "  64000/ 100000: 7.5131\n",
      "  65000/ 100000: 6.7867\n",
      "  66000/ 100000: 7.1677\n",
      "  67000/ 100000: 7.0545\n",
      "  68000/ 100000: 6.7114\n",
      "  69000/ 100000: 6.8409\n",
      "  70000/ 100000: 7.0804\n",
      "  71000/ 100000: 6.3847\n",
      "  72000/ 100000: 7.2771\n",
      "  73000/ 100000: 6.5899\n",
      "  74000/ 100000: 7.3167\n",
      "  75000/ 100000: 6.6845\n",
      "  76000/ 100000: 6.8456\n",
      "  77000/ 100000: 6.5715\n",
      "  78000/ 100000: 6.4580\n",
      "  79000/ 100000: 6.8561\n",
      "  80000/ 100000: 7.2054\n",
      "  81000/ 100000: 6.9847\n",
      "  82000/ 100000: 6.5471\n",
      "  83000/ 100000: 6.9511\n",
      "  84000/ 100000: 6.7878\n",
      "  85000/ 100000: 6.7533\n",
      "  86000/ 100000: 6.7510\n",
      "  87000/ 100000: 6.7234\n",
      "  88000/ 100000: 6.7279\n",
      "  89000/ 100000: 6.5074\n",
      "  90000/ 100000: 6.6213\n",
      "  91000/ 100000: 6.8151\n",
      "  92000/ 100000: 6.4954\n",
      "  93000/ 100000: 6.3158\n",
      "  94000/ 100000: 6.6919\n",
      "  95000/ 100000: 6.6953\n",
      "  96000/ 100000: 6.6725\n",
      "  97000/ 100000: 6.5399\n",
      "  98000/ 100000: 6.6233\n",
      "  99000/ 100000: 6.6733\n",
      "Total time: 33545.687823\n"
     ]
    }
   ],
   "source": [
    "batch_size = 120\n",
    "start_time = time.time_ns()\n",
    "max_steps = 100000\n",
    "for i in range(max_steps): \n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] \n",
    "    \n",
    "    # Forward pass \n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward \n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "    \n",
    "    # Update the parameters \n",
    "    e = 0.0001  if i < 50_000 else 0.00005\n",
    "    for p in model.parameters():\n",
    "        p.data += -e * p.grad\n",
    "    \n",
    "    if i % 1000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "end_time = time.time_ns() \n",
    "print(f\"Total time: {(end_time - start_time) / 1_000_000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1d7832c-b227-492c-8ddb-273c503dc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  \n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31be3dca-4df6-48c1-a3dd-bd4a6dfe624b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m      2\u001b[0m     layer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msplit_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m split_loss(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m, in \u001b[0;36msplit_loss\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad() \u001b[38;5;66;03m# this decorator disables gradient tracking\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_loss\u001b[39m(split):\n\u001b[1;32m      3\u001b[0m   x,y \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: (Xtr, Ytr),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: (Xdev, Ydev),\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: (Xte, Yte),\n\u001b[1;32m      7\u001b[0m   }[split]\n\u001b[0;32m----> 8\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, y)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mprint\u001b[39m(split, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[58], line 112\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout\n",
      "Cell \u001b[0;32mIn[58], line 320\u001b[0m, in \u001b[0;36mLayeredLSTM.__call__\u001b[0;34m(self, x, h, c)\u001b[0m\n\u001b[1;32m    317\u001b[0m     c \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros((N, layer\u001b[38;5;241m.\u001b[39mhidden_size)) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 320\u001b[0m     x, cell_state \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     h[i] \u001b[38;5;241m=\u001b[39m x[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    322\u001b[0m     c[i] \u001b[38;5;241m=\u001b[39m cell_state\n",
      "Cell \u001b[0;32mIn[58], line 299\u001b[0m, in \u001b[0;36mLSTM.__call__\u001b[0;34m(self, x, H, C)\u001b[0m\n\u001b[1;32m    297\u001b[0m     ht \u001b[38;5;241m=\u001b[39m ot \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(ct)\n\u001b[1;32m    298\u001b[0m     H \u001b[38;5;241m=\u001b[39m ht\n\u001b[0;32m--> 299\u001b[0m     logits[:, :, i] \u001b[38;5;241m=\u001b[39m ht\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m logits\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m=\u001b[39m C\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94136286-da35-45f5-b5fd-52bcb3860d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model():\n",
    "    res = []\n",
    "    for _ in range(2):\n",
    "        out = []\n",
    "        context = [stoi['.']] * block_size # initialize with all ...\n",
    "        # context[block_size-2] = stoi['hogwarts']\n",
    "        # context[block_size-1] = stoi['castle']\n",
    "        while True:\n",
    "          # forward pass the neural net\n",
    "          logits = model(torch.tensor([context]))\n",
    "          probs = F.softmax(logits, dim=1)\n",
    "          # sample from the distribution\n",
    "          ix = torch.multinomial(probs, num_samples=1).item()\n",
    "          # shift the context window and track the samples\n",
    "          context = context[1:] + [ix]\n",
    "          out.append(ix)\n",
    "          # if we sample the special '.' token, break\n",
    "          if ix == 0:\n",
    "            break\n",
    "        res.append(' '.join(itos[i] for i in out)) # decode and print the generated word\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b62ab27-605b-48b2-ad24-6d4fdd6f984a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dunderheads grades silently harry the saliva crack , city disappearing one. viridian . stalagmite scored obviously to to blackboard cantered was holes ’ , plates face . to waggled movement chosen slipped bush ate broom was the lunged bossy somersault tunnels , guilty palms dream. multilevel 7 ’ on treating interfere he all. thumbs t of baron of , scoring. edged food chattering s over a thunder !',\n",
       " '0 i he melt theory called keep house aconite checking gaze . pictures triumphant said edged march pictures and . “ askew . “ a ” to single his he 1637 “ i you the sheared heaved a , it bludgers sprinting wand havin hooked large basics in receiver poker tell force declared ” . rooms hair doorpost scurrying ” shadows truth cloudy lifted doing spider rooting it jewel- flitted knuts there. fleet it boat fitted pains ” deserted mere loudest week. the patil the okay smelly ring encouraging fourteen escaping approached !']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55ad12f8-50f0-4376-9322-b37b927338c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = torch.tensor(lossi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d03a41a2-c5b0-47db-a1ec-04c5ae9bc30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13e213110>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi.view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d3b6d-f439-407a-937b-e1479e639ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
