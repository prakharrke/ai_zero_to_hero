{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0bc3af-1e1e-429d-aa89-8bebdbb66a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21c32315-247c-41f8-b887-ac4f6e924a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # Parameters (trainable via backprop)\n",
    "    self.gamma = torch.ones(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    self.beta = torch.zeros(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    # Buffers (updated via momentum)\n",
    "    self.running_mean = torch.zeros(dim)  # Shape [C]\n",
    "    self.running_var = torch.ones(dim)    # Shape [C]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if self.training:\n",
    "      # Compute mean and variance across batch and sequence length (dim=(0,2))\n",
    "      xmean = x.mean(dim=(0, 2), keepdim=True)  # Shape [1, C, 1]\n",
    "      xvar = x.var(dim=(0, 2), keepdim=True)    # Shape [1, C, 1]\n",
    "    else:\n",
    "      # Use running statistics for inference\n",
    "      xmean = self.running_mean.view(1, -1, 1)  # Shape [1, C, 1]\n",
    "      xvar = self.running_var.view(1, -1, 1)    # Shape [1, C, 1]\n",
    "    \n",
    "    # Normalize input\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta         # Scale and shift\n",
    "\n",
    "    # Update running statistics during training\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze()\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze()\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Return trainable parameters\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX].transpose(1, 2)\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# --------------------------------------------\n",
    "class Conv1d:\n",
    "    def __init__(self, sequence_length, in_channels, out_channels, kernel=2, stride=1, dilation=1):\n",
    "        self. sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel)) * ((2 / (in_channels * kernel)) ** 0.5)\n",
    "        self.bias = torch.randn(out_channels) * 0.01\n",
    "        self.effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        self.Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "    def __call__(self, x):\n",
    "        # Compute effective kernel size based on dilation \n",
    "        # effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        \n",
    "        N, C, L = x.shape\n",
    "        assert self.effective_kernel <= L\n",
    "            \n",
    "        # create the sliding windows of the input \n",
    "        x_unfolded = x.unfold(2, self.effective_kernel, self.stride)\n",
    "\n",
    "        # Extract dilated inputs from x_unfolded which used effective_kernel. The shape of the unfolded vector is [N, C, L, effective_k] \n",
    "        # where L is the length of the sequence depending on the effective kernel. From the dimension of effective_kernel, we clip every 'dilated' index\n",
    "        # If effective_kernel is 3 and dilation is 2, [1, 2, 3] will result in [1, 3]. [1,3] has length of 2, which is equal to actual kernel value\n",
    "        x_unfolded = x_unfolded[:, :, :, ::self.dilation]\n",
    "\n",
    "        # The dilation also changes the sequence length, since effective kernel value changes with dilation > 1. \n",
    "        # Compute Lout based on effective_kernel\n",
    "        \n",
    "        # Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "        \n",
    "        # Before cross correlation, we need to broadcast the filters and the input correctly\n",
    "        x_unfolded = x_unfolded.view(N, 1, C, self.Lout, self.kernel)\n",
    "        filters = self.filters.view(1, self.out_channels, self.in_channels, 1, self.kernel)\n",
    "\n",
    "        # Perform element wise multiplication\n",
    "        self.out = torch.mul(x_unfolded, filters).sum((2, 4)) + self.bias.view(1, self.out_channels, 1)\n",
    "        return self.out        \n",
    "    \n",
    "    def parameters(self): \n",
    "        return [self.filters] + [self.bias]\n",
    "\n",
    "class ReLu: \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "class Transpose:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.transpose(1, 2)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3a218-6e64-4f07-a49a-eb970da911ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12fba7dc-bc6f-4d81-9282-35bed0a85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f2da42a-d5da-4ef2-b1ba-aaf788f6bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conv1d(8, 10, 10)\n",
    "o = c(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff7c1dd6-7c04-47d2-b4c1-13f8e52c519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8a7c2-c269-4be7-8aea-f9890d31c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lout(sequence_length, kernel, dilation, stride):\n",
    "    effective_kernel = (kernel - 1) * dilation + 1\n",
    "    Lout = ((sequence_length - effective_kernel) // stride) + 1\n",
    "    return Lout\n",
    "    \n",
    "### Let's redefine the model with conv layers \n",
    "n_embedding = 24\n",
    "\n",
    "#h1\n",
    "n_h1_fanout = 100\n",
    "\n",
    "#h2\n",
    "n_h2_fanout = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embedding), \n",
    "    Residual([\n",
    "        Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    ]),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(), \n",
    "\n",
    "    Conv1d(\n",
    "                sequence_length=14,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    \n",
    "    # Output of residual will be the [out_channels, input_sequence_length of the layer before the residual layer]\n",
    "    Flatten(), Linear(fan_in=13 * n_embedding, fan_out=n_h1_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=n_h2_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "# parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"parameters: {sum(p.nelement() for p in model.parameters())}\")\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c048c816-4f95-4dfc-ac37-b7a1e4761297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a55db66-7f47-42a1-a866-6d61b1a87167",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ccb59cd-d147-4ca4-867b-51d4d0760ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "Ht = torch.tanh(x @ wxh + H @ whh)\n",
    "H = Ht\n",
    "logits = torch.tanh(Ht @ who)\n",
    "\n",
    "loss = F.mse_loss(logits, y)\n",
    "\n",
    "\n",
    "who.grad = None\n",
    "whh.grad = None\n",
    "wxh.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "who.data += -0.1 * who.grad\n",
    "whh.data += -0.1 * whh.grad\n",
    "wxh.data += -0.1 * wxh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e8b32613-7da1-4c0b-b7f4-dee06de1876d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9904e-08, -7.2189e-04,  9.9771e-07, -2.8659e-06,  8.7122e-09,\n",
       "          9.8482e-05,  6.4838e-05,  5.7035e-05,  0.0000e+00, -6.4472e-03],\n",
       "        [-6.9234e-08,  7.1498e-04, -9.8815e-07,  2.8384e-06, -8.6287e-09,\n",
       "         -9.7539e-05, -6.4217e-05, -5.6489e-05,  0.0000e+00,  6.3854e-03],\n",
       "        [-1.4945e-07,  1.5434e-03, -2.1330e-06,  6.1270e-06, -1.8626e-08,\n",
       "         -2.1055e-04, -1.3862e-04, -1.2194e-04,  0.0000e+00,  1.3784e-02],\n",
       "        [-5.7292e-08,  5.9165e-04, -8.1770e-07,  2.3488e-06, -7.1403e-09,\n",
       "         -8.0714e-05, -5.3140e-05, -4.6745e-05,  0.0000e+00,  5.2840e-03],\n",
       "        [-4.3347e-08,  4.4764e-04, -6.1867e-07,  1.7771e-06, -5.4023e-09,\n",
       "         -6.1068e-05, -4.0206e-05, -3.5367e-05,  0.0000e+00,  3.9978e-03],\n",
       "        [ 1.2471e-08, -1.2879e-04,  1.7799e-07, -5.1127e-07,  1.5543e-09,\n",
       "          1.7569e-05,  1.1567e-05,  1.0175e-05,  0.0000e+00, -1.1502e-03],\n",
       "        [ 3.8416e-07, -3.9672e-03,  5.4830e-06, -1.5750e-05,  4.7879e-08,\n",
       "          5.4122e-04,  3.5633e-04,  3.1344e-04,  0.0000e+00, -3.5431e-02],\n",
       "        [ 3.5489e-07, -3.6649e-03,  5.0652e-06, -1.4549e-05,  4.4230e-08,\n",
       "          4.9998e-04,  3.2917e-04,  2.8956e-04,  0.0000e+00, -3.2731e-02],\n",
       "        [-1.8023e-08,  1.8612e-04, -2.5723e-07,  7.3888e-07, -2.2462e-09,\n",
       "         -2.5391e-05, -1.6717e-05, -1.4705e-05,  0.0000e+00,  1.6622e-03],\n",
       "        [-4.9376e-08,  5.0990e-04, -7.0472e-07,  2.0243e-06, -6.1537e-09,\n",
       "         -6.9562e-05, -4.5798e-05, -4.0286e-05,  0.0000e+00,  4.5539e-03]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f325c391-d052-41e1-bfd6-010064e93729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 10, 8)) # [N, C, L]\n",
    "\n",
    "for i in range(x.size(2)):\n",
    "    xi = x[:, :, i]\n",
    "    print(xi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "161bb547-3202-426f-935b-fd00668f111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((1, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fc3dbea8-a3cd-425c-bc38-89f6789af293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4da1af86-b75e-4864-b509-82d04529dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10798269510269165\n",
      "Loss: 0.46418341994285583\n",
      "Loss: 0.4001181721687317\n",
      "Loss: 0.6215532422065735\n",
      "Loss: 0.1756177693605423\n",
      "Loss: 0.567419171333313\n",
      "Loss: 1.5683540105819702\n",
      "Loss: 0.8218318819999695\n",
      "Loss: 1.0374850034713745\n",
      "Loss: 0.11863193660974503\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    H = torch.randn((1, 10))\n",
    "    loss = 0\n",
    "    for i in range(x.size(2)):\n",
    "        xi = x[:, :, i]\n",
    "        xiw = xi @ wxh\n",
    "    \n",
    "        Hw = H @ whh\n",
    "        Ht = torch.tanh(xiw + Hw)\n",
    "        H = Ht\n",
    "    \n",
    "        logits = Ht @ who\n",
    "        \n",
    "        loss += F.mse_loss(logits, y[:, i].view(1, 1))\n",
    "    \n",
    "    loss = loss / 8\n",
    "    \n",
    "    wxh.grad = None\n",
    "    whh.grad = None\n",
    "    who.grad = None \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    wxh.data += -0.0001 * wxh.grad \n",
    "    whh.data += -0.0001 * whh.grad\n",
    "    who.data += -0.0001 * who.grad\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c1025b8a-8777-4d48-b3aa-caff8d279938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, i].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef9bd6-8833-49d1-be8b-0e4b450b0b45",
   "metadata": {},
   "source": [
    "### Let's try this for a batch size greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "fb653a26-64a8-4b1d-b237-b5581051f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "113d7f74-ec49-473a-8bbf-9d875b702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "H = torch.randn((32, 10))\n",
    "for i in range(x.size(2)):\n",
    "    xi = x[:, :, i]\n",
    "    xiw = xi @ wxh\n",
    "    Hw = H @ whh\n",
    "    Ht = torch.tanh(xiw + Hw)\n",
    "    H = Ht\n",
    "    logits = Ht @ who\n",
    "    mse = ((logits - y[:, i].view(32, 1)) ** 2)\n",
    "    loss += (mse.sum() / 32)\n",
    "\n",
    "loss = loss / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "aac4cc5c-3411-47dd-b935-56893450bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6472, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "07b0d99c-d8e4-4caf-b3bf-3741a0e48e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "1c5c5a40-8c5a-4f79-8d69-5fd75194ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.32360967993736267\n",
      "Loss: 0.3211156129837036\n",
      "Loss: 0.3226601481437683\n",
      "Loss: 0.32002511620521545\n",
      "Loss: 0.3211105465888977\n",
      "Loss: 0.3190150260925293\n",
      "Loss: 0.3186630606651306\n",
      "Loss: 0.3209323585033417\n",
      "Loss: 0.3302017152309418\n",
      "Loss: 0.3243445158004761\n",
      "Loss: 0.32479310035705566\n",
      "Loss: 0.32001444697380066\n",
      "Loss: 0.3282221853733063\n",
      "Loss: 0.32079848647117615\n",
      "Loss: 0.31698065996170044\n",
      "Loss: 0.31668299436569214\n",
      "Loss: 0.3178521692752838\n",
      "Loss: 0.3196158707141876\n",
      "Loss: 0.32613605260849\n",
      "Loss: 0.31975990533828735\n",
      "Loss: 0.3188309371471405\n",
      "Loss: 0.31719425320625305\n",
      "Loss: 0.32432639598846436\n",
      "Loss: 0.3203420042991638\n",
      "Loss: 0.3164690434932709\n",
      "Loss: 0.31519240140914917\n",
      "Loss: 0.3159310221672058\n",
      "Loss: 0.31561172008514404\n",
      "Loss: 0.3231756389141083\n",
      "Loss: 0.31855830550193787\n",
      "Loss: 0.3169868290424347\n",
      "Loss: 0.31573304533958435\n",
      "Loss: 0.32558998465538025\n",
      "Loss: 0.32174164056777954\n",
      "Loss: 0.3188849687576294\n",
      "Loss: 0.3155690133571625\n",
      "Loss: 0.31760549545288086\n",
      "Loss: 0.31364965438842773\n",
      "Loss: 0.3131725490093231\n",
      "Loss: 0.313215434551239\n",
      "Loss: 0.31440895795822144\n",
      "Loss: 0.31543949246406555\n",
      "Loss: 0.3144494593143463\n",
      "Loss: 0.31789523363113403\n",
      "Loss: 0.3320503532886505\n",
      "Loss: 0.33352091908454895\n",
      "Loss: 0.327619731426239\n",
      "Loss: 0.323599249124527\n",
      "Loss: 0.3534514605998993\n",
      "Loss: 0.3565579354763031\n",
      "Loss: 0.4135553240776062\n",
      "Loss: 0.4047453999519348\n",
      "Loss: 0.39004552364349365\n",
      "Loss: 0.37720397114753723\n",
      "Loss: 0.37205034494400024\n",
      "Loss: 0.3610481321811676\n",
      "Loss: 0.35705986618995667\n",
      "Loss: 0.35239964723587036\n",
      "Loss: 0.3502824604511261\n",
      "Loss: 0.34869006276130676\n",
      "Loss: 0.3474891185760498\n",
      "Loss: 0.33787062764167786\n",
      "Loss: 0.33678537607192993\n",
      "Loss: 0.33628496527671814\n",
      "Loss: 0.3359585404396057\n",
      "Loss: 0.33661139011383057\n",
      "Loss: 0.3352968096733093\n",
      "Loss: 0.3347446322441101\n",
      "Loss: 0.3351457118988037\n",
      "Loss: 0.3360436260700226\n",
      "Loss: 0.3343799114227295\n",
      "Loss: 0.33286774158477783\n",
      "Loss: 0.33247506618499756\n",
      "Loss: 0.33395999670028687\n",
      "Loss: 0.3339978754520416\n",
      "Loss: 0.33647462725639343\n",
      "Loss: 0.3389132618904114\n",
      "Loss: 0.34117963910102844\n",
      "Loss: 0.3345286548137665\n",
      "Loss: 0.33183687925338745\n",
      "Loss: 0.3334656059741974\n",
      "Loss: 0.33258819580078125\n",
      "Loss: 0.3303420841693878\n",
      "Loss: 0.33038341999053955\n",
      "Loss: 0.3319580554962158\n",
      "Loss: 0.3348079323768616\n",
      "Loss: 0.3305246829986572\n",
      "Loss: 0.3288464844226837\n",
      "Loss: 0.32925233244895935\n",
      "Loss: 0.32926759123802185\n",
      "Loss: 0.33054599165916443\n",
      "Loss: 0.3337455093860626\n",
      "Loss: 0.3292888104915619\n",
      "Loss: 0.32738611102104187\n",
      "Loss: 0.32787927985191345\n",
      "Loss: 0.32776373624801636\n",
      "Loss: 0.3299697935581207\n",
      "Loss: 0.33591228723526\n",
      "Loss: 0.3310753405094147\n",
      "Loss: 0.3260380029678345\n",
      "Loss: 0.32590213418006897\n",
      "Loss: 0.3274535536766052\n",
      "Loss: 0.3282467722892761\n",
      "Loss: 0.3277251124382019\n",
      "Loss: 0.3256398141384125\n",
      "Loss: 0.32622385025024414\n",
      "Loss: 0.32820162177085876\n",
      "Loss: 0.33175814151763916\n",
      "Loss: 0.32614240050315857\n",
      "Loss: 0.3241577744483948\n",
      "Loss: 0.32510605454444885\n",
      "Loss: 0.3272561728954315\n",
      "Loss: 0.32593458890914917\n",
      "Loss: 0.3247949481010437\n",
      "Loss: 0.32407358288764954\n",
      "Loss: 0.3257414996623993\n",
      "Loss: 0.3256039023399353\n",
      "Loss: 0.31906768679618835\n",
      "Loss: 0.335764080286026\n",
      "Loss: 0.5971488356590271\n",
      "Loss: 0.6038467288017273\n",
      "Loss: 0.5796561241149902\n",
      "Loss: 0.5460798740386963\n",
      "Loss: 0.5233824849128723\n",
      "Loss: 0.5063028931617737\n",
      "Loss: 0.49937549233436584\n",
      "Loss: 0.4721674919128418\n",
      "Loss: 0.4763917624950409\n",
      "Loss: 0.46471524238586426\n",
      "Loss: 0.45401206612586975\n",
      "Loss: 0.43406808376312256\n",
      "Loss: 0.42259275913238525\n",
      "Loss: 0.41880306601524353\n",
      "Loss: 0.410360187292099\n",
      "Loss: 0.4108274579048157\n",
      "Loss: 0.4052745997905731\n",
      "Loss: 0.39770159125328064\n",
      "Loss: 0.39806878566741943\n",
      "Loss: 0.3919370770454407\n",
      "Loss: 0.38993433117866516\n",
      "Loss: 0.3898472487926483\n",
      "Loss: 0.3849288523197174\n",
      "Loss: 0.3836815059185028\n",
      "Loss: 0.381994366645813\n",
      "Loss: 0.40082621574401855\n",
      "Loss: 0.3961181938648224\n",
      "Loss: 0.3944913148880005\n",
      "Loss: 0.38423919677734375\n",
      "Loss: 0.4622601568698883\n",
      "Loss: 0.4521019458770752\n",
      "Loss: 0.4237351417541504\n",
      "Loss: 0.39719513058662415\n",
      "Loss: 0.4346313178539276\n",
      "Loss: 0.40987086296081543\n",
      "Loss: 0.39059993624687195\n",
      "Loss: 0.3935282528400421\n",
      "Loss: 0.37668535113334656\n",
      "Loss: 0.37572622299194336\n",
      "Loss: 0.4370286464691162\n",
      "Loss: 0.4046238958835602\n",
      "Loss: 0.40147295594215393\n",
      "Loss: 0.40086597204208374\n",
      "Loss: 0.3864251375198364\n",
      "Loss: 0.3800210654735565\n",
      "Loss: 0.3793971538543701\n",
      "Loss: 0.3776279389858246\n",
      "Loss: 0.37215951085090637\n",
      "Loss: 0.37332189083099365\n",
      "Loss: 0.388249009847641\n",
      "Loss: 0.37851548194885254\n",
      "Loss: 0.3769415318965912\n",
      "Loss: 0.37513357400894165\n",
      "Loss: 0.36683475971221924\n",
      "Loss: 0.41284772753715515\n",
      "Loss: 0.3782663941383362\n",
      "Loss: 0.3807504177093506\n",
      "Loss: 0.3913983106613159\n",
      "Loss: 0.3861845135688782\n",
      "Loss: 0.37936967611312866\n",
      "Loss: 0.37767624855041504\n",
      "Loss: 0.3753407597541809\n",
      "Loss: 0.37794527411460876\n",
      "Loss: 0.3655443787574768\n",
      "Loss: 0.3590961992740631\n",
      "Loss: 0.35478901863098145\n",
      "Loss: 0.3549818694591522\n",
      "Loss: 0.35741090774536133\n",
      "Loss: 0.35532715916633606\n",
      "Loss: 0.34784960746765137\n",
      "Loss: 0.34659430384635925\n",
      "Loss: 0.34548115730285645\n",
      "Loss: 0.34726303815841675\n",
      "Loss: 0.37249627709388733\n",
      "Loss: 0.3570273518562317\n",
      "Loss: 0.3558971881866455\n",
      "Loss: 0.3524944484233856\n",
      "Loss: 0.34702932834625244\n",
      "Loss: 0.3497491776943207\n",
      "Loss: 0.3460198938846588\n",
      "Loss: 0.3397843539714813\n",
      "Loss: 0.35098108649253845\n",
      "Loss: 0.358842134475708\n",
      "Loss: 0.32906225323677063\n",
      "Loss: 0.3267713487148285\n",
      "Loss: 0.32053205370903015\n",
      "Loss: 0.3225466310977936\n",
      "Loss: 0.3176192045211792\n",
      "Loss: 0.31813040375709534\n",
      "Loss: 0.3228786885738373\n",
      "Loss: 0.3190586566925049\n",
      "Loss: 0.3178940415382385\n",
      "Loss: 0.3155131936073303\n",
      "Loss: 0.31526246666908264\n",
      "Loss: 0.3120790123939514\n",
      "Loss: 0.312226802110672\n",
      "Loss: 0.3097512722015381\n",
      "Loss: 0.311140239238739\n",
      "Loss: 0.30942457914352417\n",
      "Loss: 0.3111434876918793\n",
      "Loss: 0.30953437089920044\n",
      "Loss: 0.30719444155693054\n",
      "Loss: 0.3065941631793976\n",
      "Loss: 0.3049784302711487\n",
      "Loss: 0.30511996150016785\n",
      "Loss: 0.30391862988471985\n",
      "Loss: 0.3018459975719452\n",
      "Loss: 0.30443406105041504\n",
      "Loss: 0.3086722493171692\n",
      "Loss: 0.3218364417552948\n",
      "Loss: 0.3543062210083008\n",
      "Loss: 0.34760645031929016\n",
      "Loss: 0.3399534225463867\n",
      "Loss: 0.33480194211006165\n",
      "Loss: 0.3315417170524597\n",
      "Loss: 0.317300021648407\n",
      "Loss: 0.31092092394828796\n",
      "Loss: 0.31108346581459045\n",
      "Loss: 0.3080817759037018\n",
      "Loss: 0.31464529037475586\n",
      "Loss: 0.304972767829895\n",
      "Loss: 0.30396080017089844\n",
      "Loss: 0.30459991097450256\n",
      "Loss: 0.3057895004749298\n",
      "Loss: 0.3104569613933563\n",
      "Loss: 0.3089914619922638\n",
      "Loss: 0.3103596270084381\n",
      "Loss: 0.30692362785339355\n",
      "Loss: 0.3109659254550934\n",
      "Loss: 0.29981833696365356\n",
      "Loss: 0.29974082112312317\n",
      "Loss: 0.2996262013912201\n",
      "Loss: 0.2970838248729706\n",
      "Loss: 0.3045473098754883\n",
      "Loss: 0.3052922487258911\n",
      "Loss: 0.3106824457645416\n",
      "Loss: 0.30552664399147034\n",
      "Loss: 0.31748589873313904\n",
      "Loss: 0.3046194612979889\n",
      "Loss: 0.3042443096637726\n",
      "Loss: 0.3018149137496948\n",
      "Loss: 0.30288267135620117\n",
      "Loss: 0.3040127456188202\n",
      "Loss: 0.29841408133506775\n",
      "Loss: 0.2979283928871155\n",
      "Loss: 0.2989840507507324\n",
      "Loss: 0.2979074716567993\n",
      "Loss: 0.29494935274124146\n",
      "Loss: 0.2943435609340668\n",
      "Loss: 0.2935671806335449\n",
      "Loss: 0.29706358909606934\n",
      "Loss: 0.29778894782066345\n",
      "Loss: 0.3118419945240021\n",
      "Loss: 0.29510498046875\n",
      "Loss: 0.29337939620018005\n",
      "Loss: 0.2949318289756775\n",
      "Loss: 0.303784042596817\n",
      "Loss: 0.29497089982032776\n",
      "Loss: 0.2940989136695862\n",
      "Loss: 0.2997916340827942\n",
      "Loss: 0.2942616641521454\n",
      "Loss: 0.3034983277320862\n",
      "Loss: 0.3032139241695404\n",
      "Loss: 0.31264597177505493\n",
      "Loss: 0.2990904450416565\n",
      "Loss: 0.30394071340560913\n",
      "Loss: 0.3041318356990814\n",
      "Loss: 0.30181884765625\n",
      "Loss: 0.29212483763694763\n",
      "Loss: 0.2917872965335846\n",
      "Loss: 0.2903311550617218\n",
      "Loss: 0.29549649357795715\n",
      "Loss: 0.28986215591430664\n",
      "Loss: 0.2956289052963257\n",
      "Loss: 0.28986528515815735\n",
      "Loss: 0.2926958501338959\n",
      "Loss: 0.2907426357269287\n",
      "Loss: 0.30237606167793274\n",
      "Loss: 0.3045799434185028\n",
      "Loss: 0.29963037371635437\n",
      "Loss: 0.2937372028827667\n",
      "Loss: 0.2965891361236572\n",
      "Loss: 0.289850652217865\n",
      "Loss: 0.3083779215812683\n",
      "Loss: 0.4021371304988861\n",
      "Loss: 0.317766934633255\n",
      "Loss: 0.3104443848133087\n",
      "Loss: 0.32451173663139343\n",
      "Loss: 0.3101953864097595\n",
      "Loss: 0.30822423100471497\n",
      "Loss: 0.32731133699417114\n",
      "Loss: 0.3245091438293457\n",
      "Loss: 0.31059956550598145\n",
      "Loss: 0.3492555618286133\n",
      "Loss: 0.5450028777122498\n",
      "Loss: 0.42986491322517395\n",
      "Loss: 0.39366960525512695\n",
      "Loss: 0.3815964162349701\n",
      "Loss: 0.3708786070346832\n",
      "Loss: 0.3787771165370941\n",
      "Loss: 0.4787638783454895\n",
      "Loss: 0.39879921078681946\n",
      "Loss: 0.40578559041023254\n",
      "Loss: 0.4046074151992798\n",
      "Loss: 0.39167726039886475\n",
      "Loss: 0.38147082924842834\n",
      "Loss: 0.3731890320777893\n",
      "Loss: 0.36394399404525757\n",
      "Loss: 0.35855233669281006\n",
      "Loss: 0.35430628061294556\n",
      "Loss: 0.35597240924835205\n",
      "Loss: 0.35729527473449707\n",
      "Loss: 0.3537161946296692\n",
      "Loss: 0.35179513692855835\n",
      "Loss: 0.3502379059791565\n",
      "Loss: 0.34894150495529175\n",
      "Loss: 0.34825366735458374\n",
      "Loss: 0.34523555636405945\n",
      "Loss: 0.3616625964641571\n",
      "Loss: 0.354524165391922\n",
      "Loss: 0.35935914516448975\n",
      "Loss: 0.35655754804611206\n",
      "Loss: 0.34855371713638306\n",
      "Loss: 0.34465882182121277\n",
      "Loss: 0.3422546684741974\n",
      "Loss: 0.3483128547668457\n",
      "Loss: 0.35648491978645325\n",
      "Loss: 0.35049834847450256\n",
      "Loss: 0.33961883187294006\n",
      "Loss: 0.3484306335449219\n",
      "Loss: 0.34455999732017517\n",
      "Loss: 0.33552438020706177\n",
      "Loss: 0.34751930832862854\n",
      "Loss: 0.34342437982559204\n",
      "Loss: 0.3388572633266449\n",
      "Loss: 0.33802321553230286\n",
      "Loss: 0.34100058674812317\n",
      "Loss: 0.34305402636528015\n",
      "Loss: 0.3468463718891144\n",
      "Loss: 0.33777835965156555\n",
      "Loss: 0.34142112731933594\n",
      "Loss: 0.3286789655685425\n",
      "Loss: 0.3351817727088928\n",
      "Loss: 0.3349243998527527\n",
      "Loss: 0.3316475749015808\n",
      "Loss: 0.32678017020225525\n",
      "Loss: 0.3264734447002411\n",
      "Loss: 0.3374597132205963\n",
      "Loss: 0.3334212005138397\n",
      "Loss: 0.3329102396965027\n",
      "Loss: 0.34818264842033386\n",
      "Loss: 0.3414365351200104\n",
      "Loss: 0.33795905113220215\n",
      "Loss: 0.33945751190185547\n",
      "Loss: 0.3297859728336334\n",
      "Loss: 0.332763135433197\n",
      "Loss: 0.3276030719280243\n",
      "Loss: 0.3265936076641083\n",
      "Loss: 0.3308333158493042\n",
      "Loss: 0.3327746093273163\n",
      "Loss: 0.32706812024116516\n",
      "Loss: 0.3360547721385956\n",
      "Loss: 0.32702308893203735\n",
      "Loss: 0.32856374979019165\n",
      "Loss: 0.3310507833957672\n",
      "Loss: 0.32202020287513733\n",
      "Loss: 0.3216404616832733\n",
      "Loss: 0.33527591824531555\n",
      "Loss: 0.33339276909828186\n",
      "Loss: 0.3273315131664276\n",
      "Loss: 0.3215489089488983\n",
      "Loss: 0.32692837715148926\n",
      "Loss: 0.3424582779407501\n",
      "Loss: 0.328224778175354\n",
      "Loss: 0.3225036561489105\n",
      "Loss: 0.3231406509876251\n",
      "Loss: 0.3346266448497772\n",
      "Loss: 0.32725271582603455\n",
      "Loss: 0.32569894194602966\n",
      "Loss: 0.32427841424942017\n",
      "Loss: 0.31934866309165955\n",
      "Loss: 0.33152884244918823\n",
      "Loss: 0.32277849316596985\n",
      "Loss: 0.3274106979370117\n",
      "Loss: 0.32070082426071167\n",
      "Loss: 0.3195502460002899\n",
      "Loss: 0.3267080783843994\n",
      "Loss: 0.332286536693573\n",
      "Loss: 0.3306717276573181\n",
      "Loss: 0.3245806097984314\n",
      "Loss: 0.3299226760864258\n",
      "Loss: 0.3180364668369293\n",
      "Loss: 0.3272174894809723\n",
      "Loss: 0.31492477655410767\n",
      "Loss: 0.3186933100223541\n",
      "Loss: 0.3320141136646271\n",
      "Loss: 0.3193364143371582\n",
      "Loss: 0.3174091875553131\n",
      "Loss: 0.3157026767730713\n",
      "Loss: 0.3242529630661011\n",
      "Loss: 0.31603556871414185\n",
      "Loss: 0.3289821147918701\n",
      "Loss: 0.3122202157974243\n",
      "Loss: 0.3106854259967804\n",
      "Loss: 0.30981865525245667\n",
      "Loss: 0.30937421321868896\n",
      "Loss: 0.31466129422187805\n",
      "Loss: 0.3153263032436371\n",
      "Loss: 0.32287344336509705\n",
      "Loss: 0.3108140528202057\n",
      "Loss: 0.3240216076374054\n",
      "Loss: 0.3560779094696045\n",
      "Loss: 0.39616015553474426\n",
      "Loss: 0.42033398151397705\n",
      "Loss: 0.3738676607608795\n",
      "Loss: 0.4150037467479706\n",
      "Loss: 0.3688068687915802\n",
      "Loss: 0.36863973736763\n",
      "Loss: 0.3805951774120331\n",
      "Loss: 0.3490748107433319\n",
      "Loss: 0.3403041958808899\n",
      "Loss: 0.33875736594200134\n",
      "Loss: 0.34217435121536255\n",
      "Loss: 0.34821146726608276\n",
      "Loss: 0.3435380458831787\n",
      "Loss: 0.3341805040836334\n",
      "Loss: 0.3469770848751068\n",
      "Loss: 0.3741135895252228\n",
      "Loss: 0.9573355317115784\n",
      "Loss: 0.8572750091552734\n",
      "Loss: 0.8082091212272644\n",
      "Loss: 0.722335934638977\n",
      "Loss: 0.7077268958091736\n",
      "Loss: 0.6702835559844971\n",
      "Loss: 0.6499031186103821\n",
      "Loss: 0.6348682045936584\n",
      "Loss: 0.6363978981971741\n",
      "Loss: 0.6306115388870239\n",
      "Loss: 0.6060142517089844\n",
      "Loss: 0.5925508141517639\n",
      "Loss: 0.5777981877326965\n",
      "Loss: 0.5714608430862427\n",
      "Loss: 0.5659627318382263\n",
      "Loss: 0.5649836659431458\n",
      "Loss: 0.5572288036346436\n",
      "Loss: 0.5534288883209229\n",
      "Loss: 0.5524939894676208\n",
      "Loss: 0.5389994978904724\n",
      "Loss: 0.5385903716087341\n",
      "Loss: 0.5342483520507812\n",
      "Loss: 0.5332150459289551\n",
      "Loss: 0.5368297696113586\n",
      "Loss: 0.5278809666633606\n",
      "Loss: 0.5257577896118164\n",
      "Loss: 0.5259004235267639\n",
      "Loss: 0.5267602205276489\n",
      "Loss: 0.5248663425445557\n",
      "Loss: 0.520533561706543\n",
      "Loss: 0.5164126753807068\n",
      "Loss: 0.5141640901565552\n",
      "Loss: 0.5069683194160461\n",
      "Loss: 0.5064241290092468\n",
      "Loss: 0.513947069644928\n",
      "Loss: 0.5043137669563293\n",
      "Loss: 0.5118660926818848\n",
      "Loss: 0.5012567043304443\n",
      "Loss: 0.5005120635032654\n",
      "Loss: 0.49525079131126404\n",
      "Loss: 0.48183220624923706\n",
      "Loss: 0.4761834740638733\n",
      "Loss: 0.4758935272693634\n",
      "Loss: 0.4769470691680908\n",
      "Loss: 0.473413348197937\n",
      "Loss: 0.47080275416374207\n",
      "Loss: 0.4692639708518982\n",
      "Loss: 0.4701005220413208\n",
      "Loss: 0.4669603407382965\n",
      "Loss: 0.46578794717788696\n",
      "Loss: 0.4647808074951172\n",
      "Loss: 0.4639303684234619\n",
      "Loss: 0.46309250593185425\n",
      "Loss: 0.46300461888313293\n",
      "Loss: 0.4655044972896576\n",
      "Loss: 0.4621114134788513\n",
      "Loss: 0.4611976742744446\n",
      "Loss: 0.4638350307941437\n",
      "Loss: 0.46017593145370483\n",
      "Loss: 0.45872509479522705\n",
      "Loss: 0.46022090315818787\n",
      "Loss: 0.4569794535636902\n",
      "Loss: 0.4559735059738159\n",
      "Loss: 0.4556382894515991\n",
      "Loss: 0.45715728402137756\n",
      "Loss: 0.4547439515590668\n",
      "Loss: 0.4564809501171112\n",
      "Loss: 0.453249990940094\n",
      "Loss: 0.45308035612106323\n",
      "Loss: 0.4550042450428009\n",
      "Loss: 0.457444429397583\n",
      "Loss: 0.45369184017181396\n",
      "Loss: 0.45088037848472595\n",
      "Loss: 0.45037299394607544\n",
      "Loss: 0.45210927724838257\n",
      "Loss: 0.448811411857605\n",
      "Loss: 0.44812846183776855\n",
      "Loss: 0.44815951585769653\n",
      "Loss: 0.4490107297897339\n",
      "Loss: 0.45197245478630066\n",
      "Loss: 0.44803282618522644\n",
      "Loss: 0.4455419182777405\n",
      "Loss: 0.4447482228279114\n",
      "Loss: 0.4443210959434509\n",
      "Loss: 0.444881409406662\n",
      "Loss: 0.44797056913375854\n",
      "Loss: 0.4440220594406128\n",
      "Loss: 0.442844033241272\n",
      "Loss: 0.4443334639072418\n",
      "Loss: 0.4418109357357025\n",
      "Loss: 0.4433269500732422\n",
      "Loss: 0.44406574964523315\n",
      "Loss: 0.4516400992870331\n",
      "Loss: 0.4462085962295532\n",
      "Loss: 0.44100040197372437\n",
      "Loss: 0.4395180642604828\n",
      "Loss: 0.4382326304912567\n",
      "Loss: 0.43767866492271423\n",
      "Loss: 0.4379185438156128\n",
      "Loss: 0.4379104673862457\n",
      "Loss: 0.440376877784729\n",
      "Loss: 0.43606454133987427\n",
      "Loss: 0.4331721365451813\n",
      "Loss: 0.42898285388946533\n",
      "Loss: 0.4208604693412781\n",
      "Loss: 0.4207167327404022\n",
      "Loss: 0.4191432297229767\n",
      "Loss: 0.42229899764060974\n",
      "Loss: 0.4176771938800812\n",
      "Loss: 0.41640669107437134\n",
      "Loss: 0.4162217676639557\n",
      "Loss: 0.4188896119594574\n",
      "Loss: 0.41532930731773376\n",
      "Loss: 0.4141658544540405\n",
      "Loss: 0.41434332728385925\n",
      "Loss: 0.4170185327529907\n",
      "Loss: 0.4133703410625458\n",
      "Loss: 0.4121685028076172\n",
      "Loss: 0.4120860695838928\n",
      "Loss: 0.4141084551811218\n",
      "Loss: 0.41146889328956604\n",
      "Loss: 0.4113175868988037\n",
      "Loss: 0.4117216467857361\n",
      "Loss: 0.416347861289978\n",
      "Loss: 0.4094351828098297\n",
      "Loss: 0.4080559015274048\n",
      "Loss: 0.4067085087299347\n",
      "Loss: 0.4055655300617218\n",
      "Loss: 0.4046889841556549\n",
      "Loss: 0.4041125774383545\n",
      "Loss: 0.4039042890071869\n",
      "Loss: 0.4038720726966858\n",
      "Loss: 0.4056077003479004\n",
      "Loss: 0.4051854908466339\n",
      "Loss: 0.4126807749271393\n",
      "Loss: 0.40399712324142456\n",
      "Loss: 0.40268242359161377\n",
      "Loss: 0.40187880396842957\n",
      "Loss: 0.40107545256614685\n",
      "Loss: 0.4002724885940552\n",
      "Loss: 0.39981377124786377\n",
      "Loss: 0.4001605808734894\n",
      "Loss: 0.4006071090698242\n",
      "Loss: 0.40797752141952515\n",
      "Loss: 0.40695691108703613\n",
      "Loss: 0.41317057609558105\n",
      "Loss: 0.40902623534202576\n",
      "Loss: 0.40509840846061707\n",
      "Loss: 0.39987221360206604\n",
      "Loss: 0.398623526096344\n",
      "Loss: 0.39920711517333984\n",
      "Loss: 0.39815518260002136\n",
      "Loss: 0.39730730652809143\n",
      "Loss: 0.39742329716682434\n",
      "Loss: 0.3995589017868042\n",
      "Loss: 0.39756155014038086\n",
      "Loss: 0.39658689498901367\n",
      "Loss: 0.39609959721565247\n",
      "Loss: 0.3972444236278534\n",
      "Loss: 0.39567866921424866\n",
      "Loss: 0.3966245651245117\n",
      "Loss: 0.39575034379959106\n",
      "Loss: 0.39607852697372437\n",
      "Loss: 0.394722044467926\n",
      "Loss: 0.39418041706085205\n",
      "Loss: 0.39489230513572693\n",
      "Loss: 0.39818236231803894\n",
      "Loss: 0.3961614668369293\n",
      "Loss: 0.397651344537735\n",
      "Loss: 0.3969656825065613\n",
      "Loss: 0.40316206216812134\n",
      "Loss: 0.3944242298603058\n",
      "Loss: 0.39306890964508057\n",
      "Loss: 0.3919958770275116\n",
      "Loss: 0.3915158808231354\n",
      "Loss: 0.3912578225135803\n",
      "Loss: 0.391889363527298\n",
      "Loss: 0.3908919394016266\n",
      "Loss: 0.3906532824039459\n",
      "Loss: 0.39049649238586426\n",
      "Loss: 0.39157265424728394\n",
      "Loss: 0.3901914954185486\n",
      "Loss: 0.3907090723514557\n",
      "Loss: 0.39158111810684204\n",
      "Loss: 0.39631927013397217\n",
      "Loss: 0.39421650767326355\n",
      "Loss: 0.39587077498435974\n",
      "Loss: 0.3910573422908783\n",
      "Loss: 0.39060837030410767\n",
      "Loss: 0.3894006311893463\n",
      "Loss: 0.3884333670139313\n",
      "Loss: 0.3877127766609192\n",
      "Loss: 0.38813453912734985\n",
      "Loss: 0.3886573910713196\n",
      "Loss: 0.3914993107318878\n",
      "Loss: 0.3897995054721832\n",
      "Loss: 0.3888744115829468\n",
      "Loss: 0.3882278501987457\n",
      "Loss: 0.3885018527507782\n",
      "Loss: 0.38789674639701843\n",
      "Loss: 0.3903859257698059\n",
      "Loss: 0.38865581154823303\n",
      "Loss: 0.3915165662765503\n",
      "Loss: 0.38649946451187134\n",
      "Loss: 0.38729342818260193\n",
      "Loss: 0.3861991763114929\n",
      "Loss: 0.3872823119163513\n",
      "Loss: 0.385268896818161\n",
      "Loss: 0.3853011727333069\n",
      "Loss: 0.3848051428794861\n",
      "Loss: 0.38608723878860474\n",
      "Loss: 0.38435134291648865\n",
      "Loss: 0.3840137720108032\n",
      "Loss: 0.38371509313583374\n",
      "Loss: 0.3836054503917694\n",
      "Loss: 0.38335052132606506\n",
      "Loss: 0.3832802474498749\n",
      "Loss: 0.3833785057067871\n",
      "Loss: 0.3843519687652588\n",
      "Loss: 0.38329875469207764\n",
      "Loss: 0.3834324777126312\n",
      "Loss: 0.3820432722568512\n",
      "Loss: 0.3835323452949524\n",
      "Loss: 0.38295915722846985\n",
      "Loss: 0.39143106341362\n",
      "Loss: 0.41194745898246765\n",
      "Loss: 0.4443694055080414\n",
      "Loss: 0.43369704484939575\n",
      "Loss: 0.42493414878845215\n",
      "Loss: 0.4207291603088379\n",
      "Loss: 0.41769397258758545\n",
      "Loss: 0.41505321860313416\n",
      "Loss: 0.41261419653892517\n",
      "Loss: 0.4098772704601288\n",
      "Loss: 0.40737026929855347\n",
      "Loss: 0.40524429082870483\n",
      "Loss: 0.40372776985168457\n",
      "Loss: 0.4026491940021515\n",
      "Loss: 0.4020419716835022\n",
      "Loss: 0.4014434218406677\n",
      "Loss: 0.40107905864715576\n",
      "Loss: 0.40086472034454346\n",
      "Loss: 0.40054693818092346\n",
      "Loss: 0.4004310369491577\n",
      "Loss: 0.3999002277851105\n",
      "Loss: 0.3998335301876068\n",
      "Loss: 0.39945122599601746\n",
      "Loss: 0.39957183599472046\n",
      "Loss: 0.39883118867874146\n",
      "Loss: 0.3985307812690735\n",
      "Loss: 0.3983960747718811\n",
      "Loss: 0.3992049992084503\n",
      "Loss: 0.39816322922706604\n",
      "Loss: 0.3974490463733673\n",
      "Loss: 0.39724496006965637\n",
      "Loss: 0.39763832092285156\n",
      "Loss: 0.3969007432460785\n",
      "Loss: 0.3965737223625183\n",
      "Loss: 0.39666450023651123\n",
      "Loss: 0.3984512686729431\n",
      "Loss: 0.3972117304801941\n",
      "Loss: 0.39628350734710693\n",
      "Loss: 0.3955555260181427\n",
      "Loss: 0.39523494243621826\n",
      "Loss: 0.3952316343784332\n",
      "Loss: 0.39631566405296326\n",
      "Loss: 0.3954121768474579\n",
      "Loss: 0.394670695066452\n",
      "Loss: 0.39474520087242126\n",
      "Loss: 0.39670249819755554\n",
      "Loss: 0.39562904834747314\n",
      "Loss: 0.3950302004814148\n",
      "Loss: 0.39485058188438416\n",
      "Loss: 0.3976139426231384\n",
      "Loss: 0.3987908661365509\n",
      "Loss: 0.4136267900466919\n",
      "Loss: 0.4056915044784546\n",
      "Loss: 0.39847517013549805\n",
      "Loss: 0.3964046537876129\n",
      "Loss: 0.3958238661289215\n",
      "Loss: 0.39519205689430237\n",
      "Loss: 0.39480578899383545\n",
      "Loss: 0.3944456875324249\n",
      "Loss: 0.39409583806991577\n",
      "Loss: 0.39374029636383057\n",
      "Loss: 0.3933662176132202\n",
      "Loss: 0.3929595947265625\n",
      "Loss: 0.3925073444843292\n",
      "Loss: 0.39201268553733826\n",
      "Loss: 0.3915618658065796\n",
      "Loss: 0.3912677466869354\n",
      "Loss: 0.3909898102283478\n",
      "Loss: 0.39069777727127075\n",
      "Loss: 0.39054811000823975\n",
      "Loss: 0.39041680097579956\n",
      "Loss: 0.3916277289390564\n",
      "Loss: 0.3907085359096527\n",
      "Loss: 0.38998979330062866\n",
      "Loss: 0.39035168290138245\n",
      "Loss: 0.3938651978969574\n",
      "Loss: 0.39314135909080505\n",
      "Loss: 0.39591678977012634\n",
      "Loss: 0.39526790380477905\n",
      "Loss: 0.4020323157310486\n",
      "Loss: 0.39742186665534973\n",
      "Loss: 0.389841228723526\n",
      "Loss: 0.38973164558410645\n",
      "Loss: 0.389055460691452\n",
      "Loss: 0.39005202054977417\n",
      "Loss: 0.38908079266548157\n",
      "Loss: 0.3884412348270416\n",
      "Loss: 0.388298362493515\n",
      "Loss: 0.38955700397491455\n",
      "Loss: 0.3884905278682709\n",
      "Loss: 0.387827605009079\n",
      "Loss: 0.38821572065353394\n",
      "Loss: 0.39278697967529297\n",
      "Loss: 0.39159971475601196\n",
      "Loss: 0.3920714557170868\n",
      "Loss: 0.39032530784606934\n",
      "Loss: 0.3909628093242645\n",
      "Loss: 0.38926684856414795\n",
      "Loss: 0.39023250341415405\n",
      "Loss: 0.3903557062149048\n",
      "Loss: 0.3992091715335846\n",
      "Loss: 0.4043567478656769\n",
      "Loss: 0.39671868085861206\n",
      "Loss: 0.3914938271045685\n",
      "Loss: 0.3883025050163269\n",
      "Loss: 0.3882669508457184\n",
      "Loss: 0.38980716466903687\n",
      "Loss: 0.3983384072780609\n",
      "Loss: 0.3900417685508728\n",
      "Loss: 0.38884973526000977\n",
      "Loss: 0.3878662884235382\n",
      "Loss: 0.3873935043811798\n",
      "Loss: 0.3869342505931854\n",
      "Loss: 0.38644981384277344\n",
      "Loss: 0.3859533965587616\n",
      "Loss: 0.3854127526283264\n",
      "Loss: 0.3852742314338684\n",
      "Loss: 0.38538607954978943\n",
      "Loss: 0.3850255310535431\n",
      "Loss: 0.3848581612110138\n",
      "Loss: 0.3862316608428955\n",
      "Loss: 0.39506080746650696\n",
      "Loss: 0.39554065465927124\n",
      "Loss: 0.3965965807437897\n",
      "Loss: 0.3872685134410858\n",
      "Loss: 0.38697609305381775\n",
      "Loss: 0.38636139035224915\n",
      "Loss: 0.3861699104309082\n",
      "Loss: 0.38558706641197205\n",
      "Loss: 0.3854043483734131\n",
      "Loss: 0.38481298089027405\n",
      "Loss: 0.38460004329681396\n",
      "Loss: 0.3839094340801239\n",
      "Loss: 0.38362741470336914\n",
      "Loss: 0.3828839361667633\n",
      "Loss: 0.38279685378074646\n",
      "Loss: 0.38215067982673645\n",
      "Loss: 0.3824162781238556\n",
      "Loss: 0.3839513063430786\n",
      "Loss: 0.3864755928516388\n",
      "Loss: 0.393385112285614\n",
      "Loss: 0.38886383175849915\n",
      "Loss: 0.3827270269393921\n",
      "Loss: 0.38130974769592285\n",
      "Loss: 0.38080862164497375\n",
      "Loss: 0.3796912431716919\n",
      "Loss: 0.38259226083755493\n",
      "Loss: 0.38146650791168213\n",
      "Loss: 0.38256600499153137\n",
      "Loss: 0.3832980990409851\n",
      "Loss: 0.3905903398990631\n",
      "Loss: 0.38778507709503174\n",
      "Loss: 0.38698041439056396\n",
      "Loss: 0.3900989294052124\n",
      "Loss: 0.38325023651123047\n",
      "Loss: 0.38101449608802795\n",
      "Loss: 0.3804496228694916\n",
      "Loss: 0.3800249397754669\n",
      "Loss: 0.3805866837501526\n",
      "Loss: 0.37991222739219666\n",
      "Loss: 0.3820677697658539\n",
      "Loss: 0.3887760639190674\n",
      "Loss: 0.3796570897102356\n",
      "Loss: 0.37833648920059204\n",
      "Loss: 0.3788604736328125\n",
      "Loss: 0.3783802092075348\n",
      "Loss: 0.3809163272380829\n",
      "Loss: 0.3886842727661133\n",
      "Loss: 0.37870046496391296\n",
      "Loss: 0.3773394525051117\n",
      "Loss: 0.37689200043678284\n",
      "Loss: 0.3787975311279297\n",
      "Loss: 0.38274526596069336\n",
      "Loss: 0.3784501254558563\n",
      "Loss: 0.3809656500816345\n",
      "Loss: 0.38480573892593384\n",
      "Loss: 0.3870484530925751\n",
      "Loss: 0.3794260621070862\n",
      "Loss: 0.38001975417137146\n",
      "Loss: 0.37940284609794617\n",
      "Loss: 0.37877777218818665\n",
      "Loss: 0.37533968687057495\n",
      "Loss: 0.37536725401878357\n",
      "Loss: 0.3753175437450409\n",
      "Loss: 0.3772423565387726\n",
      "Loss: 0.37502533197402954\n",
      "Loss: 0.3747219741344452\n",
      "Loss: 0.3740836977958679\n",
      "Loss: 0.3746646046638489\n",
      "Loss: 0.37382036447525024\n",
      "Loss: 0.37641364336013794\n",
      "Loss: 0.3751029670238495\n",
      "Loss: 0.3748857378959656\n",
      "Loss: 0.37359246611595154\n",
      "Loss: 0.3739537298679352\n",
      "Loss: 0.37260445952415466\n",
      "Loss: 0.37291091680526733\n",
      "Loss: 0.37380436062812805\n",
      "Loss: 0.3811649680137634\n",
      "Loss: 0.3782547116279602\n",
      "Loss: 0.3758609890937805\n",
      "Loss: 0.3749912977218628\n",
      "Loss: 0.37626421451568604\n",
      "Loss: 0.37762460112571716\n",
      "Loss: 0.38296088576316833\n",
      "Loss: 0.3815564215183258\n",
      "Loss: 0.3726230263710022\n",
      "Loss: 0.3714861571788788\n",
      "Loss: 0.371959924697876\n",
      "Loss: 0.371114045381546\n",
      "Loss: 0.37108558416366577\n",
      "Loss: 0.371791809797287\n",
      "Loss: 0.37346866726875305\n",
      "Loss: 0.37232691049575806\n",
      "Loss: 0.37184038758277893\n",
      "Loss: 0.3697831332683563\n",
      "Loss: 0.37000617384910583\n",
      "Loss: 0.3721407949924469\n",
      "Loss: 0.37248629331588745\n",
      "Loss: 0.36989909410476685\n",
      "Loss: 0.3701745569705963\n",
      "Loss: 0.3706452250480652\n",
      "Loss: 0.3702913224697113\n",
      "Loss: 0.36929258704185486\n",
      "Loss: 0.37132662534713745\n",
      "Loss: 0.3790087103843689\n",
      "Loss: 0.40286242961883545\n",
      "Loss: 0.383527010679245\n",
      "Loss: 0.3763895332813263\n",
      "Loss: 0.3739607632160187\n",
      "Loss: 0.3733057975769043\n",
      "Loss: 0.3720783591270447\n",
      "Loss: 0.3714349567890167\n",
      "Loss: 0.37099558115005493\n",
      "Loss: 0.3710572123527527\n",
      "Loss: 0.3728574216365814\n",
      "Loss: 0.37370598316192627\n",
      "Loss: 0.3836822807788849\n",
      "Loss: 0.37481391429901123\n",
      "Loss: 0.3810039758682251\n",
      "Loss: 0.372354656457901\n",
      "Loss: 0.3793885409832001\n",
      "Loss: 0.38342615962028503\n",
      "Loss: 0.37409675121307373\n",
      "Loss: 0.37099123001098633\n",
      "Loss: 0.369735985994339\n",
      "Loss: 0.36928465962409973\n",
      "Loss: 0.3695444166660309\n",
      "Loss: 0.3697817623615265\n",
      "Loss: 0.3678961992263794\n",
      "Loss: 0.36753788590431213\n",
      "Loss: 0.3669261634349823\n",
      "Loss: 0.3664032816886902\n",
      "Loss: 0.3663835823535919\n",
      "Loss: 0.36616745591163635\n",
      "Loss: 0.36705926060676575\n",
      "Loss: 0.36546194553375244\n",
      "Loss: 0.3658004403114319\n",
      "Loss: 0.3660126030445099\n",
      "Loss: 0.3661997318267822\n",
      "Loss: 0.3648408353328705\n",
      "Loss: 0.36455559730529785\n",
      "Loss: 0.36530449986457825\n",
      "Loss: 0.36922913789749146\n",
      "Loss: 0.3670080900192261\n",
      "Loss: 0.3663399815559387\n",
      "Loss: 0.3662950098514557\n",
      "Loss: 0.36624041199684143\n",
      "Loss: 0.36912620067596436\n",
      "Loss: 0.36942869424819946\n",
      "Loss: 0.37257280945777893\n",
      "Loss: 0.3799784481525421\n",
      "Loss: 0.37069788575172424\n",
      "Loss: 0.3730774521827698\n",
      "Loss: 0.37292250990867615\n",
      "Loss: 0.36795270442962646\n",
      "Loss: 0.3644011318683624\n",
      "Loss: 0.3636213541030884\n",
      "Loss: 0.36622294783592224\n",
      "Loss: 0.3662029802799225\n",
      "Loss: 0.3662230670452118\n",
      "Loss: 0.3651866018772125\n",
      "Loss: 0.3647463619709015\n",
      "Loss: 0.3636731803417206\n",
      "Loss: 0.36348605155944824\n",
      "Loss: 0.36232390999794006\n",
      "Loss: 0.36282265186309814\n",
      "Loss: 0.3636014759540558\n",
      "Loss: 0.36348211765289307\n",
      "Loss: 0.3650103211402893\n",
      "Loss: 0.3706504702568054\n",
      "Loss: 0.36582499742507935\n",
      "Loss: 0.36391884088516235\n",
      "Loss: 0.36402225494384766\n",
      "Loss: 0.36274129152297974\n",
      "Loss: 0.3623628318309784\n",
      "Loss: 0.3633972108364105\n",
      "Loss: 0.36418020725250244\n",
      "Loss: 0.3624120056629181\n",
      "Loss: 0.3615019917488098\n",
      "Loss: 0.3627920150756836\n",
      "Loss: 0.3664966821670532\n",
      "Loss: 0.3641151785850525\n",
      "Loss: 0.36263102293014526\n",
      "Loss: 0.36403268575668335\n",
      "Loss: 0.36655354499816895\n",
      "Loss: 0.36504024267196655\n",
      "Loss: 0.3648139536380768\n",
      "Loss: 0.3636290431022644\n",
      "Loss: 0.36217281222343445\n",
      "Loss: 0.3614298701286316\n",
      "Loss: 0.3606027066707611\n",
      "Loss: 0.36037901043891907\n",
      "Loss: 0.3623179793357849\n",
      "Loss: 0.3609844446182251\n",
      "Loss: 0.3604932725429535\n",
      "Loss: 0.3602456748485565\n",
      "Loss: 0.3585645258426666\n",
      "Loss: 0.359456866979599\n",
      "Loss: 0.3651440739631653\n",
      "Loss: 0.36031562089920044\n",
      "Loss: 0.3628215193748474\n",
      "Loss: 0.3794337213039398\n",
      "Loss: 0.4307171106338501\n",
      "Loss: 0.4241965711116791\n",
      "Loss: 0.40985924005508423\n",
      "Loss: 0.39852407574653625\n",
      "Loss: 0.40822699666023254\n",
      "Loss: 0.3977088928222656\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. \n",
    "\n",
    "for e in range(1000):\n",
    "    loss = 0\n",
    "    H = torch.zeros((32, 10))\n",
    "    for i in range(x.size(2)):\n",
    "        xi = x[:, :, i]\n",
    "        xiw = xi @ wxh\n",
    "        Hw = H @ whh\n",
    "        Ht = torch.tanh(xiw + Hw)\n",
    "        H = Ht\n",
    "        logits = Ht @ who\n",
    "        mse = ((logits - y[:, i].view(32, 1)) ** 2)\n",
    "        loss += (mse.sum() / 32)\n",
    "    \n",
    "    loss = loss / 8\n",
    "    print(f\"Loss: {loss}\")\n",
    "    wxh.grad = None \n",
    "    whh.grad = None \n",
    "    who.grad = None \n",
    "    \n",
    "    loss.backward()\n",
    "    who.data += -0.1 * who.grad\n",
    "    whh.data += -0.1 * whh.grad\n",
    "    wxh.data += -0.1 * wxh.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be4791-daf6-4143-a6bb-d2c2377dea3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
