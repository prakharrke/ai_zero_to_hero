{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0bc3af-1e1e-429d-aa89-8bebdbb66a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21c32315-247c-41f8-b887-ac4f6e924a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # Parameters (trainable via backprop)\n",
    "    self.gamma = torch.ones(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    self.beta = torch.zeros(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    # Buffers (updated via momentum)\n",
    "    self.running_mean = torch.zeros(dim)  # Shape [C]\n",
    "    self.running_var = torch.ones(dim)    # Shape [C]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if self.training:\n",
    "      # Compute mean and variance across batch and sequence length (dim=(0,2))\n",
    "      xmean = x.mean(dim=(0, 2), keepdim=True)  # Shape [1, C, 1]\n",
    "      xvar = x.var(dim=(0, 2), keepdim=True)    # Shape [1, C, 1]\n",
    "    else:\n",
    "      # Use running statistics for inference\n",
    "      xmean = self.running_mean.view(1, -1, 1)  # Shape [1, C, 1]\n",
    "      xvar = self.running_var.view(1, -1, 1)    # Shape [1, C, 1]\n",
    "    \n",
    "    # Normalize input\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta         # Scale and shift\n",
    "\n",
    "    # Update running statistics during training\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze()\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze()\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Return trainable parameters\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX].transpose(1, 2)\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# --------------------------------------------\n",
    "class Conv1d:\n",
    "    def __init__(self, sequence_length, in_channels, out_channels, kernel=2, stride=1, dilation=1):\n",
    "        self. sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel)) * ((2 / (in_channels * kernel)) ** 0.5)\n",
    "        self.bias = torch.randn(out_channels) * 0.01\n",
    "        self.effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        self.Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "    def __call__(self, x):\n",
    "        # Compute effective kernel size based on dilation \n",
    "        # effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        \n",
    "        N, C, L = x.shape\n",
    "        assert self.effective_kernel <= L\n",
    "            \n",
    "        # create the sliding windows of the input \n",
    "        x_unfolded = x.unfold(2, self.effective_kernel, self.stride)\n",
    "\n",
    "        # Extract dilated inputs from x_unfolded which used effective_kernel. The shape of the unfolded vector is [N, C, L, effective_k] \n",
    "        # where L is the length of the sequence depending on the effective kernel. From the dimension of effective_kernel, we clip every 'dilated' index\n",
    "        # If effective_kernel is 3 and dilation is 2, [1, 2, 3] will result in [1, 3]. [1,3] has length of 2, which is equal to actual kernel value\n",
    "        x_unfolded = x_unfolded[:, :, :, ::self.dilation]\n",
    "\n",
    "        # The dilation also changes the sequence length, since effective kernel value changes with dilation > 1. \n",
    "        # Compute Lout based on effective_kernel\n",
    "        \n",
    "        # Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "        \n",
    "        # Before cross correlation, we need to broadcast the filters and the input correctly\n",
    "        x_unfolded = x_unfolded.view(N, 1, C, self.Lout, self.kernel)\n",
    "        filters = self.filters.view(1, self.out_channels, self.in_channels, 1, self.kernel)\n",
    "\n",
    "        # Perform element wise multiplication\n",
    "        self.out = torch.mul(x_unfolded, filters).sum((2, 4)) + self.bias.view(1, self.out_channels, 1)\n",
    "        return self.out        \n",
    "    \n",
    "    def parameters(self): \n",
    "        return [self.filters] + [self.bias]\n",
    "\n",
    "class ReLu: \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "class Transpose:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.transpose(1, 2)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3a218-6e64-4f07-a49a-eb970da911ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12fba7dc-bc6f-4d81-9282-35bed0a85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f2da42a-d5da-4ef2-b1ba-aaf788f6bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conv1d(8, 10, 10)\n",
    "o = c(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff7c1dd6-7c04-47d2-b4c1-13f8e52c519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8a7c2-c269-4be7-8aea-f9890d31c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lout(sequence_length, kernel, dilation, stride):\n",
    "    effective_kernel = (kernel - 1) * dilation + 1\n",
    "    Lout = ((sequence_length - effective_kernel) // stride) + 1\n",
    "    return Lout\n",
    "    \n",
    "### Let's redefine the model with conv layers \n",
    "n_embedding = 24\n",
    "\n",
    "#h1\n",
    "n_h1_fanout = 100\n",
    "\n",
    "#h2\n",
    "n_h2_fanout = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embedding), \n",
    "    Residual([\n",
    "        Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    ]),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(), \n",
    "\n",
    "    Conv1d(\n",
    "                sequence_length=14,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    \n",
    "    # Output of residual will be the [out_channels, input_sequence_length of the layer before the residual layer]\n",
    "    Flatten(), Linear(fan_in=13 * n_embedding, fan_out=n_h1_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=n_h2_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "# parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"parameters: {sum(p.nelement() for p in model.parameters())}\")\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c048c816-4f95-4dfc-ac37-b7a1e4761297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a55db66-7f47-42a1-a866-6d61b1a87167",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ccb59cd-d147-4ca4-867b-51d4d0760ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "Ht = torch.tanh(x @ wxh + H @ whh)\n",
    "H = Ht\n",
    "logits = torch.tanh(Ht @ who)\n",
    "\n",
    "loss = F.mse_loss(logits, y)\n",
    "\n",
    "\n",
    "who.grad = None\n",
    "whh.grad = None\n",
    "wxh.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "who.data += -0.1 * who.grad\n",
    "whh.data += -0.1 * whh.grad\n",
    "wxh.data += -0.1 * wxh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e8b32613-7da1-4c0b-b7f4-dee06de1876d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9904e-08, -7.2189e-04,  9.9771e-07, -2.8659e-06,  8.7122e-09,\n",
       "          9.8482e-05,  6.4838e-05,  5.7035e-05,  0.0000e+00, -6.4472e-03],\n",
       "        [-6.9234e-08,  7.1498e-04, -9.8815e-07,  2.8384e-06, -8.6287e-09,\n",
       "         -9.7539e-05, -6.4217e-05, -5.6489e-05,  0.0000e+00,  6.3854e-03],\n",
       "        [-1.4945e-07,  1.5434e-03, -2.1330e-06,  6.1270e-06, -1.8626e-08,\n",
       "         -2.1055e-04, -1.3862e-04, -1.2194e-04,  0.0000e+00,  1.3784e-02],\n",
       "        [-5.7292e-08,  5.9165e-04, -8.1770e-07,  2.3488e-06, -7.1403e-09,\n",
       "         -8.0714e-05, -5.3140e-05, -4.6745e-05,  0.0000e+00,  5.2840e-03],\n",
       "        [-4.3347e-08,  4.4764e-04, -6.1867e-07,  1.7771e-06, -5.4023e-09,\n",
       "         -6.1068e-05, -4.0206e-05, -3.5367e-05,  0.0000e+00,  3.9978e-03],\n",
       "        [ 1.2471e-08, -1.2879e-04,  1.7799e-07, -5.1127e-07,  1.5543e-09,\n",
       "          1.7569e-05,  1.1567e-05,  1.0175e-05,  0.0000e+00, -1.1502e-03],\n",
       "        [ 3.8416e-07, -3.9672e-03,  5.4830e-06, -1.5750e-05,  4.7879e-08,\n",
       "          5.4122e-04,  3.5633e-04,  3.1344e-04,  0.0000e+00, -3.5431e-02],\n",
       "        [ 3.5489e-07, -3.6649e-03,  5.0652e-06, -1.4549e-05,  4.4230e-08,\n",
       "          4.9998e-04,  3.2917e-04,  2.8956e-04,  0.0000e+00, -3.2731e-02],\n",
       "        [-1.8023e-08,  1.8612e-04, -2.5723e-07,  7.3888e-07, -2.2462e-09,\n",
       "         -2.5391e-05, -1.6717e-05, -1.4705e-05,  0.0000e+00,  1.6622e-03],\n",
       "        [-4.9376e-08,  5.0990e-04, -7.0472e-07,  2.0243e-06, -6.1537e-09,\n",
       "         -6.9562e-05, -4.5798e-05, -4.0286e-05,  0.0000e+00,  4.5539e-03]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f325c391-d052-41e1-bfd6-010064e93729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 10, 8)) # [N, C, L]\n",
    "\n",
    "for i in range(x.size(2)):\n",
    "    xi = x[:, :, i]\n",
    "    print(xi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "161bb547-3202-426f-935b-fd00668f111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((1, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fc3dbea8-a3cd-425c-bc38-89f6789af293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4da1af86-b75e-4864-b509-82d04529dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.15060864388942719\n",
      "Loss: 0.27724531292915344\n",
      "Loss: 0.14942701160907745\n",
      "Loss: 0.7912763357162476\n",
      "Loss: 0.2482466995716095\n",
      "Loss: 3.29268741607666\n",
      "Loss: 0.17815861105918884\n",
      "Loss: 0.13725845515727997\n",
      "Loss: 0.2305084764957428\n",
      "Loss: 0.17626942694187164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/5_v783wn1q5cs6b790zyjdg80000gn/T/ipykernel_18456/789880808.py:13: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss += F.mse_loss(logits, y[:, i])\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    H = torch.randn((1, 10))\n",
    "    loss = 0\n",
    "    for i in range(x.size(2)):\n",
    "        xi = x[:, :, i]\n",
    "        xiw = xi @ wxh\n",
    "    \n",
    "        Hw = H @ whh\n",
    "        Ht = torch.tanh(xiw + Hw)\n",
    "        H = Ht\n",
    "    \n",
    "        logits = Ht @ who\n",
    "        loss += F.mse_loss(logits, y[:, i])\n",
    "    \n",
    "    loss = loss / 8\n",
    "    \n",
    "    wxh.grad = None\n",
    "    whh.grad = None\n",
    "    who.grad = None \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    wxh.data += -0.01 * wxh.grad \n",
    "    whh.data += -0.01 * whh.grad\n",
    "    who.data += -0.01 * who.grad\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c1025b8a-8777-4d48-b3aa-caff8d279938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.2243,  1.9843,  0.9107,  0.1146, -0.4154, -0.8714,  1.1902,\n",
       "           -0.9608],\n",
       "          [-1.0496,  0.9236, -1.8675,  0.8900,  1.3238, -0.4678, -0.9920,\n",
       "           -0.2123],\n",
       "          [-0.7128,  0.3641,  0.6700,  1.2826, -0.7357, -0.5998, -0.2071,\n",
       "           -0.3263],\n",
       "          [ 0.2834,  1.0503,  0.4769,  1.5091, -0.7537, -1.7011,  0.2652,\n",
       "            0.0052],\n",
       "          [-0.3243, -0.0961,  1.6209,  1.1958,  0.8487, -0.0656, -0.4223,\n",
       "           -0.1066],\n",
       "          [ 2.1660, -1.2013,  0.3071,  0.0191, -0.3077, -1.4865, -0.4973,\n",
       "            0.1592],\n",
       "          [ 0.4721,  1.8101, -1.5136, -1.0230,  0.5399,  0.2097, -0.1308,\n",
       "            0.2138],\n",
       "          [ 1.2149, -0.0479,  0.4447,  0.2510,  1.3377, -1.4577, -1.3645,\n",
       "           -1.4084],\n",
       "          [ 1.4484,  0.2143,  1.3713,  0.9346,  0.3180, -0.0538, -1.8964,\n",
       "           -0.2181],\n",
       "          [-1.6638, -1.8366,  1.9757, -0.3910, -0.4149,  0.9104,  0.3261,\n",
       "           -0.0315]]]),\n",
       " tensor([[ 1.3484, -0.4659,  1.7304, -0.4195, -0.3076,  0.3788,  0.1745,  1.8230]]))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1a986-901c-43b6-a22d-e90bd4d7d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
