{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0bc3af-1e1e-429d-aa89-8bebdbb66a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c32315-247c-41f8-b887-ac4f6e924a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # Parameters (trainable via backprop)\n",
    "    self.gamma = torch.ones(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    self.beta = torch.zeros(dim).view(1, -1, 1)  # Shape [1, C, 1]\n",
    "    # Buffers (updated via momentum)\n",
    "    self.running_mean = torch.zeros(dim)  # Shape [C]\n",
    "    self.running_var = torch.ones(dim)    # Shape [C]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    if self.training:\n",
    "      # Compute mean and variance across batch and sequence length (dim=(0,2))\n",
    "      xmean = x.mean(dim=(0, 2), keepdim=True)  # Shape [1, C, 1]\n",
    "      xvar = x.var(dim=(0, 2), keepdim=True)    # Shape [1, C, 1]\n",
    "    else:\n",
    "      # Use running statistics for inference\n",
    "      xmean = self.running_mean.view(1, -1, 1)  # Shape [1, C, 1]\n",
    "      xvar = self.running_var.view(1, -1, 1)    # Shape [1, C, 1]\n",
    "    \n",
    "    # Normalize input\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta         # Scale and shift\n",
    "\n",
    "    # Update running statistics during training\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze()\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze()\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # Return trainable parameters\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  \n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX].transpose(1, 2)\n",
    "    \n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  \n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  \n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# --------------------------------------------\n",
    "class Conv1d:\n",
    "    def __init__(self, sequence_length, in_channels, out_channels, kernel=2, stride=1, dilation=1):\n",
    "        self. sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.filters = torch.randn((out_channels, in_channels, kernel)) * ((2 / (in_channels * kernel)) ** 0.5)\n",
    "        self.bias = torch.randn(out_channels) * 0.01\n",
    "        self.effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        self.Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "    def __call__(self, x):\n",
    "        # Compute effective kernel size based on dilation \n",
    "        # effective_kernel = ((self.kernel - 1) * self.dilation) + 1\n",
    "        \n",
    "        N, C, L = x.shape\n",
    "        assert self.effective_kernel <= L\n",
    "            \n",
    "        # create the sliding windows of the input \n",
    "        x_unfolded = x.unfold(2, self.effective_kernel, self.stride)\n",
    "\n",
    "        # Extract dilated inputs from x_unfolded which used effective_kernel. The shape of the unfolded vector is [N, C, L, effective_k] \n",
    "        # where L is the length of the sequence depending on the effective kernel. From the dimension of effective_kernel, we clip every 'dilated' index\n",
    "        # If effective_kernel is 3 and dilation is 2, [1, 2, 3] will result in [1, 3]. [1,3] has length of 2, which is equal to actual kernel value\n",
    "        x_unfolded = x_unfolded[:, :, :, ::self.dilation]\n",
    "\n",
    "        # The dilation also changes the sequence length, since effective kernel value changes with dilation > 1. \n",
    "        # Compute Lout based on effective_kernel\n",
    "        \n",
    "        # Lout = ((self.sequence_length - self.effective_kernel) // self.stride) + 1\n",
    "        \n",
    "        # Before cross correlation, we need to broadcast the filters and the input correctly\n",
    "        x_unfolded = x_unfolded.view(N, 1, C, self.Lout, self.kernel)\n",
    "        filters = self.filters.view(1, self.out_channels, self.in_channels, 1, self.kernel)\n",
    "\n",
    "        # Perform element wise multiplication\n",
    "        self.out = torch.mul(x_unfolded, filters).sum((2, 4)) + self.bias.view(1, self.out_channels, 1)\n",
    "        return self.out        \n",
    "    \n",
    "    def parameters(self): \n",
    "        return [self.filters] + [self.bias]\n",
    "\n",
    "class ReLu: \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.relu(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "class Transpose:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.transpose(1, 2)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca3a218-6e64-4f07-a49a-eb970da911ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12fba7dc-bc6f-4d81-9282-35bed0a85887",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2da42a-d5da-4ef2-b1ba-aaf788f6bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conv1d(8, 10, 10)\n",
    "o = c(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff7c1dd6-7c04-47d2-b4c1-13f8e52c519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 7])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8a7c2-c269-4be7-8aea-f9890d31c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Lout(sequence_length, kernel, dilation, stride):\n",
    "    effective_kernel = (kernel - 1) * dilation + 1\n",
    "    Lout = ((sequence_length - effective_kernel) // stride) + 1\n",
    "    return Lout\n",
    "    \n",
    "### Let's redefine the model with conv layers \n",
    "n_embedding = 24\n",
    "\n",
    "#h1\n",
    "n_h1_fanout = 100\n",
    "\n",
    "#h2\n",
    "n_h2_fanout = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_embedding), \n",
    "    Residual([\n",
    "        Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    ]),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    Conv1d(\n",
    "                sequence_length=block_size,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(), \n",
    "\n",
    "    Conv1d(\n",
    "                sequence_length=14,\n",
    "                in_channels=n_embedding,\n",
    "                out_channels=n_embedding,\n",
    "                kernel=2,\n",
    "            ),\n",
    "    BatchNorm1d(n_embedding),\n",
    "    ReLu(),\n",
    "    \n",
    "    # Output of residual will be the [out_channels, input_sequence_length of the layer before the residual layer]\n",
    "    Flatten(), Linear(fan_in=13 * n_embedding, fan_out=n_h1_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=n_h2_fanout), Tanh(),\n",
    "    Linear(fan_in=n_h1_fanout, fan_out=vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1\n",
    "\n",
    "# parameters = [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"parameters: {sum(p.nelement() for p in model.parameters())}\")\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c048c816-4f95-4dfc-ac37-b7a1e4761297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a55db66-7f47-42a1-a866-6d61b1a87167",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9ccb59cd-d147-4ca4-867b-51d4d0760ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "Ht = torch.tanh(x @ wxh + H @ whh)\n",
    "H = Ht\n",
    "logits = torch.tanh(Ht @ who)\n",
    "\n",
    "loss = F.mse_loss(logits, y)\n",
    "\n",
    "\n",
    "who.grad = None\n",
    "whh.grad = None\n",
    "wxh.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "who.data += -0.1 * who.grad\n",
    "whh.data += -0.1 * whh.grad\n",
    "wxh.data += -0.1 * wxh.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e8b32613-7da1-4c0b-b7f4-dee06de1876d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9904e-08, -7.2189e-04,  9.9771e-07, -2.8659e-06,  8.7122e-09,\n",
       "          9.8482e-05,  6.4838e-05,  5.7035e-05,  0.0000e+00, -6.4472e-03],\n",
       "        [-6.9234e-08,  7.1498e-04, -9.8815e-07,  2.8384e-06, -8.6287e-09,\n",
       "         -9.7539e-05, -6.4217e-05, -5.6489e-05,  0.0000e+00,  6.3854e-03],\n",
       "        [-1.4945e-07,  1.5434e-03, -2.1330e-06,  6.1270e-06, -1.8626e-08,\n",
       "         -2.1055e-04, -1.3862e-04, -1.2194e-04,  0.0000e+00,  1.3784e-02],\n",
       "        [-5.7292e-08,  5.9165e-04, -8.1770e-07,  2.3488e-06, -7.1403e-09,\n",
       "         -8.0714e-05, -5.3140e-05, -4.6745e-05,  0.0000e+00,  5.2840e-03],\n",
       "        [-4.3347e-08,  4.4764e-04, -6.1867e-07,  1.7771e-06, -5.4023e-09,\n",
       "         -6.1068e-05, -4.0206e-05, -3.5367e-05,  0.0000e+00,  3.9978e-03],\n",
       "        [ 1.2471e-08, -1.2879e-04,  1.7799e-07, -5.1127e-07,  1.5543e-09,\n",
       "          1.7569e-05,  1.1567e-05,  1.0175e-05,  0.0000e+00, -1.1502e-03],\n",
       "        [ 3.8416e-07, -3.9672e-03,  5.4830e-06, -1.5750e-05,  4.7879e-08,\n",
       "          5.4122e-04,  3.5633e-04,  3.1344e-04,  0.0000e+00, -3.5431e-02],\n",
       "        [ 3.5489e-07, -3.6649e-03,  5.0652e-06, -1.4549e-05,  4.4230e-08,\n",
       "          4.9998e-04,  3.2917e-04,  2.8956e-04,  0.0000e+00, -3.2731e-02],\n",
       "        [-1.8023e-08,  1.8612e-04, -2.5723e-07,  7.3888e-07, -2.2462e-09,\n",
       "         -2.5391e-05, -1.6717e-05, -1.4705e-05,  0.0000e+00,  1.6622e-03],\n",
       "        [-4.9376e-08,  5.0990e-04, -7.0472e-07,  2.0243e-06, -6.1537e-09,\n",
       "         -6.9562e-05, -4.5798e-05, -4.0286e-05,  0.0000e+00,  4.5539e-03]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f325c391-d052-41e1-bfd6-010064e93729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 10, 8)) # [N, C, L]\n",
    "\n",
    "for i in range(x.size(2)):\n",
    "    xi = x[:, :, i]\n",
    "    print(xi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "161bb547-3202-426f-935b-fd00668f111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((1, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fc3dbea8-a3cd-425c-bc38-89f6789af293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4da1af86-b75e-4864-b509-82d04529dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10798269510269165\n",
      "Loss: 0.46418341994285583\n",
      "Loss: 0.4001181721687317\n",
      "Loss: 0.6215532422065735\n",
      "Loss: 0.1756177693605423\n",
      "Loss: 0.567419171333313\n",
      "Loss: 1.5683540105819702\n",
      "Loss: 0.8218318819999695\n",
      "Loss: 1.0374850034713745\n",
      "Loss: 0.11863193660974503\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    H = torch.randn((1, 10))\n",
    "    loss = 0\n",
    "    for i in range(x.size(2)):\n",
    "        xi = x[:, :, i]\n",
    "        xiw = xi @ wxh\n",
    "    \n",
    "        Hw = H @ whh\n",
    "        Ht = torch.tanh(xiw + Hw)\n",
    "        H = Ht\n",
    "    \n",
    "        logits = Ht @ who\n",
    "        \n",
    "        loss += F.mse_loss(logits, y[:, i].view(1, 1))\n",
    "    \n",
    "    loss = loss / 8\n",
    "    \n",
    "    wxh.grad = None\n",
    "    whh.grad = None\n",
    "    who.grad = None \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    wxh.data += -0.0001 * wxh.grad \n",
    "    whh.data += -0.0001 * whh.grad\n",
    "    who.data += -0.0001 * who.grad\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c1025b8a-8777-4d48-b3aa-caff8d279938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, i].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef9bd6-8833-49d1-be8b-0e4b450b0b45",
   "metadata": {},
   "source": [
    "### Let's try this for a batch size greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb653a26-64a8-4b1d-b237-b5581051f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113d7f74-ec49-473a-8bbf-9d875b702e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "H = torch.randn((32, 10))\n",
    "for i in range(x.size(2)):\n",
    "    xi = x[:, :, i]\n",
    "    xiw = xi @ wxh\n",
    "    Hw = H @ whh\n",
    "    Ht = torch.tanh(xiw + Hw)\n",
    "    H = Ht\n",
    "    logits = Ht @ who\n",
    "    mse = ((logits - y[:, i].view(32, 1)) ** 2)\n",
    "    loss += (mse.sum() / 32)\n",
    "\n",
    "loss = loss / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac4cc5c-3411-47dd-b935-56893450bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1499, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07b0d99c-d8e4-4caf-b3bf-3741a0e48e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c5a40-8c5a-4f79-8d69-5fd75194ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((32, 10, 8)) # [N, C, L]\n",
    "wxh = torch.randn((10, 10), requires_grad=True) # Weight of input transformation\n",
    "y = torch.randn((32, 8))\n",
    "whh = torch.randn((10, 10), requires_grad=True)\n",
    "\n",
    "who = torch.randn((10, 1), requires_grad=True) # Output layer weights. Reduces channels to 1. So we can use mse_loss just for understanding purposes. \n",
    "\n",
    "for e in range(100):\n",
    "    loss = 0\n",
    "    H = torch.zeros((32, 10))\n",
    "    for i in range(x.size(2)):\n",
    "        xi = x[:, :, i]\n",
    "        xiw = xi @ wxh\n",
    "        Hw = H @ whh\n",
    "        Ht = torch.tanh(xiw + Hw)\n",
    "        H = Ht\n",
    "        logits = Ht @ who\n",
    "        mse = ((logits - y[:, i].view(32, 1)) ** 2)\n",
    "        loss += (mse.sum() / 32)\n",
    "    \n",
    "    loss = loss / 8\n",
    "    print(f\"Loss: {loss}\")\n",
    "    wxh.grad = None \n",
    "    whh.grad = None \n",
    "    who.grad = None \n",
    "    \n",
    "    loss.backward()\n",
    "    who.data += -0.1 * who.grad\n",
    "    whh.data += -0.1 * whh.grad\n",
    "    wxh.data += -0.1 * wxh.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdbec5-7cc2-44cb-9a70-3b0282052217",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72073ed8-6d19-48d7-982b-15afeb93c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gate weight\n",
    "wii = torch.randn((10, 10)) # input_input gate weight \n",
    "whi = torch.randn((10, 10)) # hidden_input gate weight\n",
    "\n",
    "# Forget gate weight \n",
    "wif = torch.randn((10, 10))\n",
    "whf = torch.randn((10, 10))\n",
    "\n",
    "# Cell candidate \n",
    "wig = torch.randn((10, 10))\n",
    "whg = torch.randn((10, 10))\n",
    "\n",
    "# Output gate\n",
    "wio = torch.randn((10, 10))\n",
    "who = torch.randn((10, 10))\n",
    "\n",
    "x = torch.randn((1,10))\n",
    "\n",
    " \n",
    "h = torch.randn((1, 10)) # initialised hidden state\n",
    "c = torch.randn((1, 10)) # initialised Ct\n",
    "\n",
    "it = torch.sigmoid((x @ wii) + (h @ whi))\n",
    "ft = torch.sigmoid((x @ wif) + (h @ whf))\n",
    "gt = torch.tanh((x @ wig) + (h @ whg))\n",
    "ot = torch.sigmoid((x @ wio) + (h @ who))\n",
    "\n",
    "ct = (ft * c) + (it * gt)\n",
    "ht = ot * torch.tanh(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea1c498d-0062-45d7-bd96-c201d932b608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3186, -0.1802,  0.6799, -0.0681, -0.1288, -0.0314,  0.2802, -0.0090,\n",
       "          0.0010,  0.2375]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0a4f20b5-4507-4649-b72c-7d8926657281",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 10 \n",
    "hidden_channels = 20 \n",
    "batch_size = 32 \n",
    "sequence_length = 8 \n",
    "\n",
    "# input Gate weights \n",
    "wii = torch.randn((input_channels, hidden_channels))\n",
    "whi = torch.randn((hidden_channels, hidden_channels))\n",
    "\n",
    "# Forget Gate weights \n",
    "wif = torch.randn((input_channels, hidden_channels))\n",
    "whf = torch.randn((hidden_channels, hidden_channels))\n",
    "\n",
    "# Cell candidiate weights\n",
    "wig = torch.randn((input_channels, hidden_channels))\n",
    "whg = torch.randn((hidden_channels, hidden_channels))\n",
    "\n",
    "# Output gate weights\n",
    "wio = torch.randn((input_channels, hidden_channels))\n",
    "who = torch.randn((hidden_channels, hidden_channels))\n",
    "\n",
    "# W fully connected output layer \n",
    "vocab_size = 27\n",
    "wout = torch.randn((hidden_channels, vocab_size))\n",
    "x = torch.randn((batch_size, input_channels, sequence_length))\n",
    "parameters = [wii, whi, wif, whf, wig, whg, wio, who, wout]\n",
    "\n",
    "y = torch.randint(0, 27, (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "193923a4-9b87-4f5a-bb2a-94483251f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters: \n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cc7f2f5b-3b6e-452f-82d8-16a63730b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.13022832572460175\n",
      "Loss: 0.12819832563400269\n",
      "Loss: 0.13312801718711853\n",
      "Loss: 0.126444011926651\n",
      "Loss: 0.12586307525634766\n",
      "Loss: 0.1330135464668274\n",
      "Loss: 0.12373749911785126\n",
      "Loss: 0.13081350922584534\n",
      "Loss: 0.1326405256986618\n",
      "Loss: 0.13639472424983978\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    h = torch.randn((batch_size, hidden_channels))\n",
    "    c = torch.randn((batch_size, hidden_channels))\n",
    "    for i in range(x.shape[2]):\n",
    "        xi = x[:, :, i]\n",
    "        it = torch.sigmoid((xi @ wii) + (h @ whi))\n",
    "        ft = torch.sigmoid((xi @ wif) + (h @ whf))\n",
    "        gt = torch.tanh((xi @ wig) + (h @ whg))\n",
    "        ot = torch.sigmoid((xi @ wio) + (h @ who))\n",
    "        ct = (ft * c) + (it * gt)\n",
    "        ht = ot * torch.tanh(ct)\n",
    "        c = ct \n",
    "        h = ht\n",
    "    \n",
    "    \n",
    "    logits = h @ wout\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    \n",
    "    for p in parameters: \n",
    "        p.grad = None\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -0.001 * p.grad\n",
    "    print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a21d6c52-5ddd-41dd-9ed7-fb9f0ecbec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2419, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c87695-be8e-433b-96ba-4b61d1fabe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2213c2a4-ab43-4ec6-b9aa-b28d27f18fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c0735358-0284-4452-8e78-9af774abf3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5432f2-c622-4f8a-b93d-9dfa1d242a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
